<!DOCTYPE html>
<html class="client-nojs" dir="ltr" lang="en">
<head>
<meta charset="utf-8"/>
<title>Multi-armed bandit - Wikipedia</title>
<script>document.documentElement.className=document.documentElement.className.replace(/(^|\s)client-nojs(\s|$)/,"$1client-js$2");RLCONF={"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Multi-armed_bandit","wgTitle":"Multi-armed bandit","wgCurRevisionId":891844157,"wgRevisionId":891844157,"wgArticleId":2854828,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 errors: chapter ignored","CS1 maint: Multiple names: authors list","All articles with unsourced statements","Articles with unsourced statements from March 2015","Sequential methods","Sequential experiments","Stochastic optimization","Machine learning"],"wgBreakFrames":!1,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September",
"October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Multi-armed_bandit","wgRelevantArticleId":2854828,"wgRequestId":"XPjvqwpAMFkAAHUr64cAAAAW","wgCSPNonce":!1,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgPoweredByHHVM":!0,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgWikibaseItemId":"Q2882343","wgCentralAuthMobileDomain":!1,
"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","ext.3d.styles":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","ext.math.scripts","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice",
"ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.tokens@0tffind",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link href="/w/load.php?lang=en&amp;modules=ext.3d.styles%7Cext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta content="" name="ResourceLoaderDynamicStyles"/>
<link href="/w/load.php?lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<link href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector" rel="stylesheet"/>
<meta content="MediaWiki 1.34.0-wmf.7" name="generator"/>
<meta content="origin" name="referrer"/>
<meta content="origin-when-crossorigin" name="referrer"/>
<meta content="origin-when-cross-origin" name="referrer"/>
<meta content="https://upload.wikimedia.org/wikipedia/commons/8/82/Las_Vegas_slot_machines.jpg" property="og:image"/>
<link href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Multi-armed_bandit" rel="alternate"/>
<link href="/w/index.php?title=Multi-armed_bandit&amp;action=edit" rel="alternate" title="Edit this page" type="application/x-wiki"/>
<link href="/w/index.php?title=Multi-armed_bandit&amp;action=edit" rel="edit" title="Edit this page"/>
<link href="/static/apple-touch/wikipedia.png" rel="apple-touch-icon"/>
<link href="/static/favicon/wikipedia.ico" rel="shortcut icon"/>
<link href="/w/opensearch_desc.php" rel="search" title="Wikipedia (en)" type="application/opensearchdescription+xml"/>
<link href="//en.wikipedia.org/w/api.php?action=rsd" rel="EditURI" type="application/rsd+xml"/>
<link href="//creativecommons.org/licenses/by-sa/3.0/" rel="license"/>
<link href="https://en.wikipedia.org/wiki/Multi-armed_bandit" rel="canonical"/>
<link href="//login.wikimedia.org" rel="dns-prefetch"/>
<link href="//meta.wikimedia.org" rel="dns-prefetch"/>
<!--[if lt IE 9]><script src="/w/load.php?lang=qqx&amp;modules=html5shiv&amp;only=scripts&amp;skin=fallback&amp;sync=1"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Multi-armed_bandit rootpage-Multi-armed_bandit skin-vector action-view">
<div class="noprint" id="mw-page-base"></div>
<div class="noprint" id="mw-head-base"></div>
<div class="mw-body" id="content" role="main">
<a id="top"></a>
<div class="mw-body-content" id="siteNotice"><!-- CentralNotice --></div>
<div class="mw-indicators mw-body-content">
</div>
<h1 class="firstHeading" id="firstHeading" lang="en">Multi-armed bandit</h1>
<div class="mw-body-content" id="bodyContent">
<div class="noprint" id="siteSub">From Wikipedia, the free encyclopedia</div>
<div id="contentSub"></div>
<div id="jump-to-nav"></div>
<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
<a class="mw-jump-link" href="#p-search">Jump to search</a>
<div class="mw-content-ltr" dir="ltr" id="mw-content-text" lang="en"><div class="mw-parser-output"><div class="thumb tright"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:Las_Vegas_slot_machines.jpg"><img alt="" class="thumbimage" data-file-height="768" data-file-width="1100" decoding="async" height="154" src="//upload.wikimedia.org/wikipedia/commons/thumb/8/82/Las_Vegas_slot_machines.jpg/220px-Las_Vegas_slot_machines.jpg" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/82/Las_Vegas_slot_machines.jpg/330px-Las_Vegas_slot_machines.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/82/Las_Vegas_slot_machines.jpg/440px-Las_Vegas_slot_machines.jpg 2x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Las_Vegas_slot_machines.jpg" title="Enlarge"></a></div>A row of slot machines in Las Vegas</div></div></div>
<p>In <a href="/wiki/Probability_theory" title="Probability theory">probability theory</a>, the <b>multi-armed bandit problem</b> (sometimes called the <b><i>K</i>-<sup class="reference" id="cite_ref-doi10.1023/A:1013689704352_1-0"><a href="#cite_note-doi10.1023/A:1013689704352-1">[1]</a></sup> or <i>N</i>-armed bandit problem</b><sup class="reference" id="cite_ref-2"><a href="#cite_note-2">[2]</a></sup>) is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice.<sup class="reference" id="cite_ref-Gittins89_3-0"><a href="#cite_note-Gittins89-3">[3]</a></sup><sup class="reference" id="cite_ref-BF_4-0"><a href="#cite_note-BF-4">[4]</a></sup> This is a classic <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">reinforcement learning</a> problem that exemplifies the exploration-exploitation tradeoff dilemma. The name comes from imagining a <a class="mw-redirect" href="/wiki/Gambler" title="Gambler">gambler</a> at a row of <a class="mw-redirect" href="/wiki/Slot_machines" title="Slot machines">slot machines</a> (sometimes known as "one-armed bandits"), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine.<sup class="reference" id="cite_ref-weber_5-0"><a href="#cite_note-weber-5">[5]</a></sup> The multi-armed bandit problem also falls into the broad category of <a href="/wiki/Stochastic_scheduling" title="Stochastic scheduling">stochastic scheduling</a>.
</p><p>In the problem, each machine provides a random reward from a <a href="/wiki/Probability_distribution" title="Probability distribution">probability distribution</a> specific to that machine. The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls.<sup class="reference" id="cite_ref-Gittins89_3-1"><a href="#cite_note-Gittins89-3">[3]</a></sup><sup class="reference" id="cite_ref-BF_4-1"><a href="#cite_note-BF-4">[4]</a></sup> The crucial tradeoff the gambler faces at each trial is between "exploitation" of the machine that has the highest expected payoff and "exploration" to get more <a href="/wiki/Bayes%27_theorem" title="Bayes' theorem">information</a> about the expected payoffs of the other machines. The trade-off between exploration and exploitation is also faced in <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>. In practice, multi-armed bandits have been used to model problems such as managing research projects in a large organization like a science foundation or a <a href="/wiki/Pharmaceutical_industry" title="Pharmaceutical industry">pharmaceutical company</a>.<sup class="reference" id="cite_ref-Gittins89_3-2"><a href="#cite_note-Gittins89-3">[3]</a></sup><sup class="reference" id="cite_ref-BF_4-2"><a href="#cite_note-BF-4">[4]</a></sup> In early versions of the problem, the gambler begins with no initial knowledge about the machines.
</p><p><a href="/wiki/Herbert_Robbins" title="Herbert Robbins">Herbert Robbins</a> in 1952, realizing the importance of the problem, constructed convergent population selection strategies in "some aspects of the sequential design of experiments".<sup class="reference" id="cite_ref-6"><a href="#cite_note-6">[6]</a></sup> A theorem, the <a href="/wiki/Gittins_index" title="Gittins index">Gittins index</a>, first published by <a href="/wiki/John_C._Gittins" title="John C. Gittins">John C. Gittins</a>, gives an optimal policy for maximizing the expected discounted reward.<sup class="reference" id="cite_ref-7"><a href="#cite_note-7">[7]</a></sup>
</p>
<div class="toc" id="toc"><input class="toctogglecheckbox" id="toctogglecheckbox" role="button" style="display:none" type="checkbox"/><div class="toctitle" dir="ltr" lang="en"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Empirical_motivation"><span class="tocnumber">1</span> <span class="toctext">Empirical motivation</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#The_multi-armed_bandit_model"><span class="tocnumber">2</span> <span class="toctext">The multi-armed bandit model</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Variations"><span class="tocnumber">3</span> <span class="toctext">Variations</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Bandit_strategies"><span class="tocnumber">4</span> <span class="toctext">Bandit strategies</span></a>
<ul>
<li class="toclevel-2 tocsection-5"><a href="#Optimal_solutions"><span class="tocnumber">4.1</span> <span class="toctext">Optimal solutions</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="#Approximate_solutions"><span class="tocnumber">4.2</span> <span class="toctext">Approximate solutions</span></a>
<ul>
<li class="toclevel-3 tocsection-7"><a href="#Semi-uniform_strategies"><span class="tocnumber">4.2.1</span> <span class="toctext">Semi-uniform strategies</span></a></li>
<li class="toclevel-3 tocsection-8"><a href="#Probability_matching_strategies"><span class="tocnumber">4.2.2</span> <span class="toctext">Probability matching strategies</span></a></li>
<li class="toclevel-3 tocsection-9"><a href="#Pricing_strategies"><span class="tocnumber">4.2.3</span> <span class="toctext">Pricing strategies</span></a></li>
<li class="toclevel-3 tocsection-10"><a href="#Strategies_with_ethical_constraints"><span class="tocnumber">4.2.4</span> <span class="toctext">Strategies with ethical constraints</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-11"><a href="#Contextual_bandit"><span class="tocnumber">5</span> <span class="toctext">Contextual bandit</span></a>
<ul>
<li class="toclevel-2 tocsection-12"><a href="#Approximate_solutions_for_contextual_bandit"><span class="tocnumber">5.1</span> <span class="toctext">Approximate solutions for contextual bandit</span></a>
<ul>
<li class="toclevel-3 tocsection-13"><a href="#Online_linear_classifier"><span class="tocnumber">5.1.1</span> <span class="toctext">Online linear classifier</span></a></li>
<li class="toclevel-3 tocsection-14"><a href="#Online_non-linear_classifier"><span class="tocnumber">5.1.2</span> <span class="toctext">Online non-linear classifier</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-15"><a href="#Constrained_contextual_bandit"><span class="tocnumber">5.2</span> <span class="toctext">Constrained contextual bandit</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-16"><a href="#Adversarial_bandit"><span class="tocnumber">6</span> <span class="toctext">Adversarial bandit</span></a>
<ul>
<li class="toclevel-2 tocsection-17"><a href="#Example:_iterated_prisoner's_dilemma"><span class="tocnumber">6.1</span> <span class="toctext">Example: iterated prisoner's dilemma</span></a></li>
<li class="toclevel-2 tocsection-18"><a href="#Approximate_solutions_2"><span class="tocnumber">6.2</span> <span class="toctext">Approximate solutions</span></a>
<ul>
<li class="toclevel-3 tocsection-19"><a href="#Exp3[43]"><span class="tocnumber">6.2.1</span> <span class="toctext">Exp3<sup>[43]</sup></span></a>
<ul>
<li class="toclevel-4 tocsection-20"><a href="#Algorithm"><span class="tocnumber">6.2.1.1</span> <span class="toctext">Algorithm</span></a></li>
<li class="toclevel-4 tocsection-21"><a href="#Explanation"><span class="tocnumber">6.2.1.2</span> <span class="toctext">Explanation</span></a></li>
<li class="toclevel-4 tocsection-22"><a href="#Regret_analysis"><span class="tocnumber">6.2.1.3</span> <span class="toctext">Regret analysis</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-23"><a href="#Follow_the_perturbed_leader_(FPL)_algorithm"><span class="tocnumber">6.2.2</span> <span class="toctext">Follow the perturbed leader (FPL) algorithm</span></a>
<ul>
<li class="toclevel-4 tocsection-24"><a href="#Algorithm_2"><span class="tocnumber">6.2.2.1</span> <span class="toctext">Algorithm</span></a></li>
<li class="toclevel-4 tocsection-25"><a href="#Explanation_2"><span class="tocnumber">6.2.2.2</span> <span class="toctext">Explanation</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-26"><a href="#Exp3_vs_FPL"><span class="tocnumber">6.2.3</span> <span class="toctext">Exp3 vs FPL</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-27"><a href="#Infinite-armed_bandit"><span class="tocnumber">7</span> <span class="toctext">Infinite-armed bandit</span></a></li>
<li class="toclevel-1 tocsection-28"><a href="#Non-stationary_bandit"><span class="tocnumber">8</span> <span class="toctext">Non-stationary bandit</span></a></li>
<li class="toclevel-1 tocsection-29"><a href="#Other_variants"><span class="tocnumber">9</span> <span class="toctext">Other variants</span></a>
<ul>
<li class="toclevel-2 tocsection-30"><a href="#Dueling_bandit"><span class="tocnumber">9.1</span> <span class="toctext">Dueling bandit</span></a></li>
<li class="toclevel-2 tocsection-31"><a href="#Collaborative_bandit"><span class="tocnumber">9.2</span> <span class="toctext">Collaborative bandit</span></a></li>
<li class="toclevel-2 tocsection-32"><a href="#Combinatorial_bandit"><span class="tocnumber">9.3</span> <span class="toctext">Combinatorial bandit</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-33"><a href="#See_also"><span class="tocnumber">10</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-34"><a href="#References"><span class="tocnumber">11</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-35"><a href="#Further_reading"><span class="tocnumber">12</span> <span class="toctext">Further reading</span></a></li>
<li class="toclevel-1 tocsection-36"><a href="#External_links"><span class="tocnumber">13</span> <span class="toctext">External links</span></a></li>
</ul>
</div>
<h2><span class="mw-headline" id="Empirical_motivation">Empirical motivation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=1" title="Edit section: Empirical motivation">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:The_Jet_Propulsion_Laboratory_(9416811752).jpg"><img alt="" class="thumbimage" data-file-height="1488" data-file-width="2319" decoding="async" height="141" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/26/The_Jet_Propulsion_Laboratory_%289416811752%29.jpg/220px-The_Jet_Propulsion_Laboratory_%289416811752%29.jpg" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/26/The_Jet_Propulsion_Laboratory_%289416811752%29.jpg/330px-The_Jet_Propulsion_Laboratory_%289416811752%29.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/26/The_Jet_Propulsion_Laboratory_%289416811752%29.jpg/440px-The_Jet_Propulsion_Laboratory_%289416811752%29.jpg 2x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:The_Jet_Propulsion_Laboratory_(9416811752).jpg" title="Enlarge"></a></div>How to distribute a given budget among these research departments to maximize results?</div></div></div>
<p>The multi-armed bandit problem models an agent that simultaneously attempts to acquire new knowledge (called "exploration") and optimize their decisions based on existing knowledge (called "exploitation"). The agent attempts to balance these competing tasks in order to maximize their total value over the period of time considered. There are many practical applications of the bandit model, for example:
</p>
<ul><li><a href="/wiki/Clinical_trial" title="Clinical trial">clinical trials</a> investigating the effects of different experimental treatments while minimizing patient losses,<sup class="reference" id="cite_ref-Gittins89_3-3"><a href="#cite_note-Gittins89-3">[3]</a></sup><sup class="reference" id="cite_ref-BF_4-3"><a href="#cite_note-BF-4">[4]</a></sup><sup class="reference" id="cite_ref-WHP_8-0"><a href="#cite_note-WHP-8">[8]</a></sup><sup class="reference" id="cite_ref-KD_9-0"><a href="#cite_note-KD-9">[9]</a></sup></li>
<li><a class="mw-redirect" href="/wiki/Adaptive_routing" title="Adaptive routing">adaptive routing</a> efforts for minimizing delays in a network,</li>
<li><a href="/wiki/Portfolio_(finance)" title="Portfolio (finance)">financial portfolio design</a><sup class="reference" id="cite_ref-BrochuHoffmandeFreitas_10-0"><a href="#cite_note-BrochuHoffmandeFreitas-10">[10]</a></sup><sup class="reference" id="cite_ref-ShenWangJiangZha_11-0"><a href="#cite_note-ShenWangJiangZha-11">[11]</a></sup></li></ul>
<p>In these practical examples, the problem requires balancing reward maximization based on the knowledge already acquired with attempting new actions to further increase knowledge. This is known as the <i>exploitation vs. exploration tradeoff</i> in <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>.
</p><p>The model has also been used to control dynamic allocation of resources to different projects, answering the question of which project to work on, given uncertainty about the difficulty and payoff of each possibility.<sup class="reference" id="cite_ref-farias2011irrevocable_12-0"><a href="#cite_note-farias2011irrevocable-12">[12]</a></sup>
</p><p>Originally considered by Allied scientists in <a href="/wiki/World_War_II" title="World War II">World War II</a>, it proved so intractable that, according to <a href="/wiki/Peter_Whittle_(mathematician)" title="Peter Whittle (mathematician)">Peter Whittle</a>, the problem was proposed to be dropped over <a href="/wiki/Germany" title="Germany">Germany</a> so that German scientists could also waste their time on it.<sup class="reference" id="cite_ref-Whittle79_13-0"><a href="#cite_note-Whittle79-13">[13]</a></sup>
</p><p>The version of the problem now commonly analyzed was formulated by <a href="/wiki/Herbert_Robbins" title="Herbert Robbins">Herbert Robbins</a> in 1952.
</p>
<h2><span class="mw-headline" id="The_multi-armed_bandit_model">The multi-armed bandit model</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=2" title="Edit section: The multi-armed bandit model">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The multi-armed bandit (short: <i>bandit</i> or MAB) can be seen as a set of real <a href="/wiki/Probability_distribution" title="Probability distribution">distributions</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle B=\{R_{1},\dots ,R_{K}\}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>B</mi>
<mo>=</mo>
<mo fence="false" stretchy="false">{</mo>
<msub>
<mi>R</mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>R</mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>K</mi>
</mrow>
</msub>
<mo fence="false" stretchy="false">}</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle B=\{R_{1},\dots ,R_{K}\}}</annotation>
</semantics>
</math></span><img alt="B=\{R_{1},\dots ,R_{K}\}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e67c275c3f89e55686789c105cfa69f9d4f09859" style="vertical-align: -0.838ex; width:18.641ex; height:2.843ex;"/></span>, each distribution being  associated with the rewards delivered by one of the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle K\in \mathbb {N} ^{+}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>K</mi>
<mo>∈<!-- ∈ --></mo>
<msup>
<mrow class="MJX-TeXAtom-ORD">
<mi mathvariant="double-struck">N</mi>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mo>+</mo>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle K\in \mathbb {N} ^{+}}</annotation>
</semantics>
</math></span><img alt="K\in \mathbb {N} ^{+}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d90b7afbf12f15744a3b8b2163ccf551ef01a9aa" style="vertical-align: -0.338ex; width:8.095ex; height:2.509ex;"/></span> levers. Let <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mu _{1},\dots ,\mu _{K}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mi>μ<!-- μ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mn>1</mn>
</mrow>
</msub>
<mo>,</mo>
<mo>…<!-- … --></mo>
<mo>,</mo>
<msub>
<mi>μ<!-- μ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>K</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mu _{1},\dots ,\mu _{K}}</annotation>
</semantics>
</math></span><img alt="\mu _{1},\dots ,\mu _{K}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d2c289c5a590fb89a7cabda5eaa4b69efd2782a2" style="vertical-align: -0.838ex; width:10.729ex; height:2.176ex;"/></span> be the mean values associated with these reward distributions. The gambler iteratively plays one lever per round and observes the associated reward. The objective is to maximize the sum of the collected rewards. The horizon <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle H}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>H</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle H}</annotation>
</semantics>
</math></span><img alt="H" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/75a9edddcca2f782014371f75dca39d7e13a9c1b" style="vertical-align: -0.338ex; width:2.064ex; height:2.176ex;"/></span> is the number of rounds that remain to be played. The bandit problem is formally equivalent to a one-state <a href="/wiki/Markov_decision_process" title="Markov decision process">Markov decision process</a>. The <a href="/wiki/Regret_(decision_theory)" title="Regret (decision theory)">regret</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \rho }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ρ<!-- ρ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \rho }</annotation>
</semantics>
</math></span><img alt="\rho " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f7d439671d1289b6a816e6af7a304be40608d64" style="vertical-align: -0.838ex; width:1.202ex; height:2.176ex;"/></span> after <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle T}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>T</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle T}</annotation>
</semantics>
</math></span><img alt="T" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ec7200acd984a1d3a3d7dc455e262fbe54f7f6e0" style="vertical-align: -0.338ex; width:1.636ex; height:2.176ex;"/></span> rounds is defined as the expected difference between the reward sum associated with an optimal strategy and the sum of the collected rewards: 
</p><p><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \rho =T\mu ^{*}-\sum _{t=1}^{T}{\widehat {r}}_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ρ<!-- ρ --></mi>
<mo>=</mo>
<mi>T</mi>
<msup>
<mi>μ<!-- μ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
<mo>−<!-- − --></mo>
<munderover>
<mo>∑<!-- ∑ --></mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
<mo>=</mo>
<mn>1</mn>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>T</mi>
</mrow>
</munderover>
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>r</mi>
<mo>^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \rho =T\mu ^{*}-\sum _{t=1}^{T}{\widehat {r}}_{t}}</annotation>
</semantics>
</math></span><img alt="\rho =T\mu ^{*}-\sum _{t=1}^{T}{\widehat {r}}_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/374d9c4611ca7fff2d61176207a547f16f5877a8" style="vertical-align: -3.005ex; width:17.093ex; height:7.343ex;"/></span>,
</p><p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mu ^{*}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>μ<!-- μ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mu ^{*}}</annotation>
</semantics>
</math></span><img alt="\mu ^{*}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/670d0d4db6668c13d249c92fb99c34d2a9f236f7" style="vertical-align: -0.838ex; width:2.456ex; height:2.843ex;"/></span> is the maximal reward mean, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \mu ^{*}=\max _{k}\{\mu _{k}\}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msup>
<mi>μ<!-- μ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>∗<!-- ∗ --></mo>
</mrow>
</msup>
<mo>=</mo>
<munder>
<mo form="prefix" movablelimits="true">max</mo>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
</mrow>
</munder>
<mo fence="false" stretchy="false">{</mo>
<msub>
<mi>μ<!-- μ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mi>k</mi>
</mrow>
</msub>
<mo fence="false" stretchy="false">}</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \mu ^{*}=\max _{k}\{\mu _{k}\}}</annotation>
</semantics>
</math></span><img alt="\mu ^{*}=\max _{k}\{\mu _{k}\}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e39c290f0835d49cc7ad055a1558c92a9473ca20" style="vertical-align: -2.005ex; width:14.696ex; height:4.009ex;"/></span>, and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\widehat {r}}_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<msub>
<mrow class="MJX-TeXAtom-ORD">
<mrow class="MJX-TeXAtom-ORD">
<mover>
<mi>r</mi>
<mo>^<!-- ^ --></mo>
</mover>
</mrow>
</mrow>
<mrow class="MJX-TeXAtom-ORD">
<mi>t</mi>
</mrow>
</msub>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle {\widehat {r}}_{t}}</annotation>
</semantics>
</math></span><img alt="{\widehat {r}}_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a9ade3337c1462d0b6b5e25badce32bbf9f8fff5" style="vertical-align: -0.671ex; width:2.118ex; height:2.509ex;"/></span> is the reward in round <i>t</i>.
</p><p>A <i>zero-regret strategy</i> is a strategy whose average regret per round <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \rho /T}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ρ<!-- ρ --></mi>
<mrow class="MJX-TeXAtom-ORD">
<mo>/</mo>
</mrow>
<mi>T</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \rho /T}</annotation>
</semantics>
</math></span><img alt="\rho /T" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d840953c8a665b101ddc3b09c702ca89eb6eef39" style="vertical-align: -0.838ex; width:4.001ex; height:2.843ex;"/></span> tends to zero with probability 1 when the number of played rounds tends to infinity.<sup class="reference" id="cite_ref-Vermorel2005_14-0"><a href="#cite_note-Vermorel2005-14">[14]</a></sup> Intuitively, zero-regret strategies are guaranteed to converge to a (not necessarily unique) optimal strategy if enough rounds are played.
</p>
<h2><span class="mw-headline" id="Variations">Variations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=3" title="Edit section: Variations">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A common formulation is the <i>Binary multi-armed bandit</i> or <i>Bernoulli multi-armed bandit,</i> which issues a reward of one with probability <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle p}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>p</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle p}</annotation>
</semantics>
</math></span><img alt="p" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;"/></span>, and otherwise a reward of zero.
</p><p>Another formulation of the multi-armed bandit has each arm representing an independent Markov machine. Each time a particular arm is played, the state of that machine advances to a new one, chosen according to the Markov state evolution probabilities. There is a reward depending on the current state of the machine. In a generalisation called the "restless bandit problem", the states of non-played arms can also evolve over time.<sup class="reference" id="cite_ref-Whittle88_15-0"><a href="#cite_note-Whittle88-15">[15]</a></sup> There has also been discussion of systems where the number of choices (about which arm to play) increases over time.<sup class="reference" id="cite_ref-Whittle81_16-0"><a href="#cite_note-Whittle81-16">[16]</a></sup>
</p><p>Computer science researchers have studied multi-armed bandits under worst-case assumptions, obtaining algorithms to minimize regret in both finite and infinite (<a class="mw-redirect" href="/wiki/Asymptotic" title="Asymptotic">asymptotic</a>) time horizons for both stochastic<sup class="reference" id="cite_ref-doi10.1023/A:1013689704352_1-1"><a href="#cite_note-doi10.1023/A:1013689704352-1">[1]</a></sup> and non-stochastic<sup class="reference" id="cite_ref-17"><a href="#cite_note-17">[17]</a></sup> arm payoffs.
</p>
<h2><span class="mw-headline" id="Bandit_strategies">Bandit strategies</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=4" title="Edit section: Bandit strategies">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A major breakthrough was the construction of optimal population selection strategies, or policies (that possess uniformly maximum convergence rate  to the population with highest mean) in the work described below.
</p>
<h3><span class="mw-headline" id="Optimal_solutions">Optimal solutions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=5" title="Edit section: Optimal solutions">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In the paper "Asymptotically efficient adaptive allocation rules", Lai and Robbins<sup class="reference" id="cite_ref-18"><a href="#cite_note-18">[18]</a></sup>  (following papers of Robbins and his co-workers going back to Robbins in the year 1952) constructed convergent population selection policies that possess the fastest rate of convergence (to the population with highest mean) for the case that the population reward distributions are the one-parameter exponential family.  Then, in <a href="/wiki/Michael_Katehakis" title="Michael Katehakis">Katehakis</a> and <a href="/wiki/Herbert_Robbins" title="Herbert Robbins">Robbins</a><sup class="reference" id="cite_ref-19"><a href="#cite_note-19">[19]</a></sup> simplifications of the policy and the main proof were given for the case of normal populations with known variances. The next notable progress was obtained by Burnetas and <a href="/wiki/Michael_Katehakis" title="Michael Katehakis">Katehakis</a>  in the paper "Optimal adaptive policies for sequential allocation problems",<sup class="reference" id="cite_ref-20"><a href="#cite_note-20">[20]</a></sup> where index based policies  with uniformly maximum convergence rate were constructed, under more general conditions that include the case in which the distributions of outcomes from each population depend on a vector of unknown parameters. Burnetas and Katehakis (1996) also provided an explicit solution for the important case in which the distributions of outcomes follow arbitrary (i.e., non-parametric) discrete, univariate distributions.
</p><p>Later in "Optimal adaptive policies for Markov decision processes"<sup class="reference" id="cite_ref-21"><a href="#cite_note-21">[21]</a></sup>  Burnetas and Katehakis studied the much larger model of Markov Decision Processes under partial information,  where the transition law and/or the expected one period rewards may depend on unknown parameters. In this work the explicit form for a class of adaptive policies that possess uniformly maximum convergence rate  properties for the total expected finite horizon reward, were constructed under sufficient assumptions of finite state-action spaces and irreducibility of the transition law. A main feature of these policies is that the choice of actions, at each state and time period, is based on indices that are inflations of the right-hand side of the estimated average reward optimality equations. These inflations have recently been called the optimistic approach in the work of Tewari and Bartlett,<sup class="reference" id="cite_ref-22"><a href="#cite_note-22">[22]</a></sup> Ortner<sup class="reference" id="cite_ref-23"><a href="#cite_note-23">[23]</a></sup> Filippi,  Cappé, and Garivier,<sup class="reference" id="cite_ref-24"><a href="#cite_note-24">[24]</a></sup> and Honda and Takemura.<sup class="reference" id="cite_ref-25"><a href="#cite_note-25">[25]</a></sup>
</p>
<h3><span class="mw-headline" id="Approximate_solutions">Approximate solutions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=6" title="Edit section: Approximate solutions">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Many strategies exist which provide an approximate solution to the bandit problem, and can be put into the four broad categories detailed below.
</p>
<h4><span class="mw-headline" id="Semi-uniform_strategies">Semi-uniform strategies</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=7" title="Edit section: Semi-uniform strategies">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Semi-uniform strategies were the earliest (and simplest) strategies discovered to approximately solve the bandit problem. All those strategies have in common a <a href="/wiki/Greedy_algorithm" title="Greedy algorithm">greedy</a> behavior where the <i>best</i> lever (based on previous observations) is always pulled except when a (uniformly) random action is taken.
</p>
<ul><li><b>Epsilon-greedy strategy</b>:<sup class="reference" id="cite_ref-26"><a href="#cite_note-26">[26]</a></sup> The best lever is selected for a proportion <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle 1-\epsilon }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mn>1</mn>
<mo>−<!-- − --></mo>
<mi>ϵ<!-- ϵ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle 1-\epsilon }</annotation>
</semantics>
</math></span><img alt="1-\epsilon " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/57f9b07affe80ff61cdc4f2e47977c8421a59c73" style="vertical-align: -0.505ex; width:4.947ex; height:2.343ex;"/></span> of the trials, and a lever is selected at random (with uniform probability) for a proportion <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \epsilon }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ϵ<!-- ϵ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \epsilon }</annotation>
</semantics>
</math></span><img alt="\epsilon " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3837cad72483d97bcdde49c85d3b7b859fb3fd2" style="vertical-align: -0.338ex; width:0.944ex; height:1.676ex;"/></span>. A typical parameter value might be <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \epsilon =0.1}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ϵ<!-- ϵ --></mi>
<mo>=</mo>
<mn>0.1</mn>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \epsilon =0.1}</annotation>
</semantics>
</math></span><img alt="\epsilon =0.1" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/71f42be1fff0327aea7ea21875c1fe08c3336685" style="vertical-align: -0.338ex; width:7.014ex; height:2.176ex;"/></span>, but this can vary widely depending on circumstances and predilections.</li>
<li><b>Epsilon-first strategy</b><sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (March 2015)">citation needed</span></a></i>]</sup>: A pure exploration phase is followed by a pure exploitation phase. For <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle N}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>N</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle N}</annotation>
</semantics>
</math></span><img alt="N" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f5e3890c981ae85503089652feb48b191b57aae3" style="vertical-align: -0.338ex; width:2.064ex; height:2.176ex;"/></span> trials in total, the exploration phase occupies <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \epsilon N}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ϵ<!-- ϵ --></mi>
<mi>N</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \epsilon N}</annotation>
</semantics>
</math></span><img alt="\epsilon N" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e324e9387177f1d79321b241d810338925072954" style="vertical-align: -0.338ex; width:3.008ex; height:2.176ex;"/></span> trials and the exploitation phase <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle (1-\epsilon )N}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo>−<!-- − --></mo>
<mi>ϵ<!-- ϵ --></mi>
<mo stretchy="false">)</mo>
<mi>N</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle (1-\epsilon )N}</annotation>
</semantics>
</math></span><img alt="(1-\epsilon )N" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2db747031b50331ca469d363e23f5b1010187f15" style="vertical-align: -0.838ex; width:8.82ex; height:2.843ex;"/></span> trials. During the exploration phase, a lever is randomly selected (with uniform probability); during the exploitation phase, the best lever is always selected.</li>
<li><b>Epsilon-decreasing strategy</b><sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (March 2015)">citation needed</span></a></i>]</sup>: Similar to the epsilon-greedy strategy, except that the value of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \epsilon }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ϵ<!-- ϵ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \epsilon }</annotation>
</semantics>
</math></span><img alt="\epsilon " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3837cad72483d97bcdde49c85d3b7b859fb3fd2" style="vertical-align: -0.338ex; width:0.944ex; height:1.676ex;"/></span> decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish.</li>
<li><b>Adaptive epsilon-greedy strategy based on value differences (VDBE)</b>: Similar to the epsilon-decreasing strategy, except that  epsilon is reduced on basis of the learning progress instead of manual tuning (Tokic, 2010).<sup class="reference" id="cite_ref-Tokic2010_27-0"><a href="#cite_note-Tokic2010-27">[27]</a></sup> High fluctuations in the value estimates lead to a high epsilon (high exploration, low exploitation); low fluctuations to a low epsilon (low exploration, high exploitation). Further improvements can be achieved by a <a href="/wiki/Softmax_function" title="Softmax function">softmax</a>-weighted action selection in case of exploratory actions (Tokic &amp; Palm, 2011).<sup class="reference" id="cite_ref-TokicPalm2011_28-0"><a href="#cite_note-TokicPalm2011-28">[28]</a></sup></li>
<li><b>Contextual-Epsilon-greedy strategy</b>: Similar to the epsilon-greedy strategy, except that the value of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \epsilon }" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>ϵ<!-- ϵ --></mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle \epsilon }</annotation>
</semantics>
</math></span><img alt="\epsilon " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c3837cad72483d97bcdde49c85d3b7b859fb3fd2" style="vertical-align: -0.338ex; width:0.944ex; height:1.676ex;"/></span> is computed regarding the situation in experiment processes, which let the algorithm be Context-Aware. It is based on dynamic exploration/exploitation and can adaptively balance the two aspects by deciding which situation is most relevant for exploration or exploitation, resulting in highly explorative behavior when the situation is not critical and highly exploitative behavior at critical situation.<sup class="reference" id="cite_ref-Bouneffouf2012_29-0"><a href="#cite_note-Bouneffouf2012-29">[29]</a></sup></li></ul>
<h4><span class="mw-headline" id="Probability_matching_strategies">Probability matching strategies</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=8" title="Edit section: Probability matching strategies">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Probability matching strategies reflect the idea that the number of pulls for a given lever should <i>match</i> its actual probability of being the optimal lever.  Probability matching strategies are also known as <a href="/wiki/Thompson_sampling" title="Thompson sampling">Thompson sampling</a> or Bayesian Bandits,<sup class="reference" id="cite_ref-Scott2010_30-0"><a href="#cite_note-Scott2010-30">[30]</a></sup> and are surprisingly easy to implement if you can sample from the posterior for the mean value of each alternative.
</p><p>Probability matching strategies also admit solutions to so-called contextual bandit problems.
</p>
<h4><span class="mw-headline" id="Pricing_strategies">Pricing strategies</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=9" title="Edit section: Pricing strategies">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Pricing strategies establish a <i>price</i> for each lever. For example, as illustrated with the POKER algorithm,<sup class="reference" id="cite_ref-Vermorel2005_14-1"><a href="#cite_note-Vermorel2005-14">[14]</a></sup> the price can be the sum of the expected reward plus an estimation of extra future rewards that will gain through the additional knowledge. The lever of highest price is always pulled.
</p>
<h4><span class="mw-headline" id="Strategies_with_ethical_constraints">Strategies with ethical constraints</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=10" title="Edit section: Strategies with ethical constraints">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>These strategies minimize the assignment of any patient to an inferior arm (<a href="/wiki/Medical_ethics" title="Medical ethics">"physician's duty"</a>).  In a typical case, they minimize expected successes lost (ESL), that is, the expected number of favorable outcomes that were missed because of assignment to an arm later proved to be inferior.  Another version minimizes resources wasted on any inferior, more expensive, treatment.<sup class="reference" id="cite_ref-WHP_8-1"><a href="#cite_note-WHP-8">[8]</a></sup>
</p>
<h2><span class="mw-headline" id="Contextual_bandit">Contextual bandit</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=11" title="Edit section: Contextual bandit">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A particularly useful version of the multi-armed bandit is the contextual multi-armed bandit problem. In this problem, in each iteration an agent has to choose between arms. Before making the choice, the agent sees a d-dimensional feature vector (context vector),
associated with the current iteration. The learner uses these context vectors along with the rewards of the arms played in the past to make the choice of the arm to play in
the current iteration. Over time, the learner's aim is to collect enough information about how the context vectors and rewards relate to each other, so that it can predict the next best arm to play by looking at the feature vectors.<sup class="reference" id="cite_ref-Langford2008_31-0"><a href="#cite_note-Langford2008-31">[31]</a></sup>
</p>
<h3><span class="mw-headline" id="Approximate_solutions_for_contextual_bandit">Approximate solutions for contextual bandit</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=12" title="Edit section: Approximate solutions for contextual bandit">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Many strategies exist that provide an approximate solution to the contextual bandit problem, and can be put into two broad categories detailed below.
</p>
<h4><span class="mw-headline" id="Online_linear_classifier">Online linear classifier</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=13" title="Edit section: Online linear classifier">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>LinUCB <i>(Upper Confidence Bound)</i> algorithm</b>: the authors assume a linear dependency between the expected reward of an action and its context and model the representation space using a set of linear predictors.</li>
<li><b>LinRel (Linear Associative Reinforcement Learning) algorithm</b>: Similar to LinUCB, but utilizes <a class="mw-redirect" href="/wiki/Singular-value_decomposition" title="Singular-value decomposition">Singular-value decomposition</a> rather than <a class="mw-redirect" href="/wiki/Ridge_regression" title="Ridge regression">Ridge regression</a> to obtain a better estimate of confidence.<sup class="reference" id="cite_ref-32"><a href="#cite_note-32">[32]</a></sup><sup class="reference" id="cite_ref-33"><a href="#cite_note-33">[33]</a></sup></li></ul>
<h4><span class="mw-headline" id="Online_non-linear_classifier">Online non-linear classifier</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=14" title="Edit section: Online non-linear classifier">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<ul><li><b>UCBogram algorithm</b>: The nonlinear reward functions are estimated using a piecewise constant estimator called a <i>regressogram</i> in <a href="/wiki/Nonparametric_regression" title="Nonparametric regression">Nonparametric regression</a>. Then, UCB is employed on each constant piece. Successive refinements of the partition of the context space are scheduled or chosen adaptively.<sup class="reference" id="cite_ref-RigZee10_34-0"><a href="#cite_note-RigZee10-34">[34]</a></sup><sup class="reference" id="cite_ref-slivkins11_35-0"><a href="#cite_note-slivkins11-35">[35]</a></sup><sup class="reference" id="cite_ref-PerRig13_36-0"><a href="#cite_note-PerRig13-36">[36]</a></sup></li>
<li><b>NeuralBandit algorithm</b>:  In this algorithm several neural networks are trained to modelize the value of rewards knowing the context, and it uses a multi-experts approach to choose online the parameters of multi-layer perceptrons.<sup class="reference" id="cite_ref-Robin2014_37-0"><a href="#cite_note-Robin2014-37">[37]</a></sup></li>
<li><b>KernelUCB algorithm</b>: a kernelized non-linear version of linearUCB, with efficient implementation and finite-time analysis.<sup class="reference" id="cite_ref-Valko2014_38-0"><a href="#cite_note-Valko2014-38">[38]</a></sup></li>
<li><b>Bandit Forest algorithm</b>: a random forest is built and analyzed w.r.t the random forest built knowing the joint distribution of contexts and rewards.<sup class="reference" id="cite_ref-39"><a href="#cite_note-39">[39]</a></sup></li></ul>
<h3><span class="mw-headline" id="Constrained_contextual_bandit">Constrained contextual bandit</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=15" title="Edit section: Constrained contextual bandit">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In practice, there is usually a cost associated with the resource consumed by each action and the total cost is limited by a budget in many applications such as crowdsourcing and clinical trials. Constrained contextual bandit (CCB) is such a model that considers both the time and budget constraints in a multi-armed bandit setting.
A. Badanidiyuru et al.<sup class="reference" id="cite_ref-Badanidiyuru2014COLT_40-0"><a href="#cite_note-Badanidiyuru2014COLT-40">[40]</a></sup> first studied contextual bandits with budget constraints, also referred to as Resourceful Contextual Bandits, and show that a <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O({\sqrt {T}})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<msqrt>
<mi>T</mi>
</msqrt>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O({\sqrt {T}})}</annotation>
</semantics>
</math></span><img alt="O({\sqrt {T}})" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b08c799ec97aa1891429c5deea1bea86b1c8701e" style="vertical-align: -0.838ex; width:7.155ex; height:3.176ex;"/></span> regret is achievable. However, their work focuses on a finite set of policies, and the algorithm is computationally inefficient.
</p>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a class="image" href="/wiki/File:Framework_of_UCB-ALP_for_Constrained_Contextual_Bandits.jpg"><img alt="" class="thumbimage" data-file-height="863" data-file-width="1199" decoding="async" height="158" src="//upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Framework_of_UCB-ALP_for_Constrained_Contextual_Bandits.jpg/220px-Framework_of_UCB-ALP_for_Constrained_Contextual_Bandits.jpg" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Framework_of_UCB-ALP_for_Constrained_Contextual_Bandits.jpg/330px-Framework_of_UCB-ALP_for_Constrained_Contextual_Bandits.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Framework_of_UCB-ALP_for_Constrained_Contextual_Bandits.jpg/440px-Framework_of_UCB-ALP_for_Constrained_Contextual_Bandits.jpg 2x" width="220"/></a> <div class="thumbcaption"><div class="magnify"><a class="internal" href="/wiki/File:Framework_of_UCB-ALP_for_Constrained_Contextual_Bandits.jpg" title="Enlarge"></a></div>Framework of UCB-ALP for constrained contextual bandits</div></div></div>
<p>A simple algorithm with logarithmic regret is proposed in:<sup class="reference" id="cite_ref-Wu2015UCBALP_41-0"><a href="#cite_note-Wu2015UCBALP-41">[41]</a></sup>
</p>
<ul><li><b>UCB-ALP algorithm</b>: The framework of UCB-ALP is shown in the right figure. UCB-ALP is a simple algorithm that combines the UCB method with an Adaptive Linear Programming (ALP) algorithm, and can be easily deployed in practical systems. It is the first work that show how to achieve logarithmic regret in constrained contextual bandits. Although<sup class="reference" id="cite_ref-Wu2015UCBALP_41-1"><a href="#cite_note-Wu2015UCBALP-41">[41]</a></sup> is devoted to a special case with single budget constraint and fixed cost, the results shed light on the design and analysis of algorithms for more general CCB problems.</li></ul>
<h2><span class="mw-headline" id="Adversarial_bandit">Adversarial bandit</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=16" title="Edit section: Adversarial bandit">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Another variant of the multi-armed bandit problem is called the adversarial bandit, first introduced by Auer and Cesa-Bianchi (1998). In this variant, at each iteration, an agent chooses an arm and an adversary simultaneously chooses the payoff structure for each arm. This is one of the strongest generalizations of the bandit problem<sup class="reference" id="cite_ref-42"><a href="#cite_note-42">[42]</a></sup> as it removes all assumptions of the distribution and a solution to the adversarial bandit problem is a generalized solution to the more specific bandit problems.
</p>
<h3><span id="Example:_iterated_prisoner.27s_dilemma"></span><span class="mw-headline" id="Example:_iterated_prisoner's_dilemma">Example: iterated prisoner's dilemma</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=17" title="Edit section: Example: iterated prisoner's dilemma">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>An example often considered for adversarial bandits is the <a class="mw-redirect" href="/wiki/Iterated_prisoner%27s_dilemma" title="Iterated prisoner's dilemma">iterated prisoner's dilemma</a>. In this example, each adversary has two arms to pull. They can either Deny or Confess. Standard stochastic bandit algorithms don't work very well with this iterations. For example, if the opponent cooperates in the first 100 rounds, defects for the next 200, then cooperate in the following 300, etc. Then algorithms such as UCB won't be able to react very quickly to these changes. This is because after a certain point sub-optimal arms are rarely pulled to limit exploration and focus on exploitation. When the environment changes the algorithm is unable to adapt or may not even detect the change.
</p>
<h3><span class="mw-headline" id="Approximate_solutions_2">Approximate solutions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=18" title="Edit section: Approximate solutions">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span id="Exp3.5B43.5D"></span><span class="mw-headline" id="Exp3[43]">Exp3<sup class="reference" id="cite_ref-43"><a href="#cite_note-43">[43]</a></sup></span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=19" title="Edit section: Exp3[43]">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<h5><span class="mw-headline" id="Algorithm">Algorithm</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=20" title="Edit section: Algorithm">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<pre> <b>Parameters:</b> Real <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \gamma \in (0,1]}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>γ<!-- γ --></mi>
        <mo>∈<!-- ∈ --></mo>
        <mo stretchy="false">(</mo>
        <mn>0</mn>
        <mo>,</mo>
        <mn>1</mn>
        <mo stretchy="false">]</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \gamma \in (0,1]}</annotation>
  </semantics>
</math></span><img alt="{\displaystyle \gamma \in (0,1]}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3dc19e2d373af486a2a9fe5dc8c3b749cf5756db" style="vertical-align: -0.838ex; width:9.013ex; height:2.843ex;"/></span>
 
 <b>Initialisation:</b> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \omega _{i}(1)=1}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>ω<!-- ω --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \omega _{i}(1)=1}</annotation>
  </semantics>
</math></span><img alt="{\displaystyle \omega _{i}(1)=1}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0564ff3a4c0bb900291c1aad0d4e129d3e898a58" style="vertical-align: -0.838ex; width:9.478ex; height:2.843ex;"/></span> for <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i=1,...,K}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mo>.</mo>
        <mo>.</mo>
        <mo>.</mo>
        <mo>,</mo>
        <mi>K</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle i=1,...,K}</annotation>
  </semantics>
</math></span><img alt="{\displaystyle i=1,...,K}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4fcf403a99f3f343092b1b23411804c0c0aa874c" style="vertical-align: -0.671ex; width:12.299ex; height:2.509ex;"/></span>
 
 <b>For each</b> t = 1, 2, ..., T
  1. Set <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle p_{i}(t)=(1-\gamma ){\frac {\omega _{i}(t)}{\sum _{j=1}^{K}\omega _{j}(t)}}+{\frac {\gamma }{K}}}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>−<!-- − --></mo>
        <mi>γ<!-- γ --></mi>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <msub>
                <mi>ω<!-- ω --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
              </msub>
              <mo stretchy="false">(</mo>
              <mi>t</mi>
              <mo stretchy="false">)</mo>
            </mrow>
            <mrow>
              <munderover>
                <mo>∑<!-- ∑ --></mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>K</mi>
                </mrow>
              </munderover>
              <msub>
                <mi>ω<!-- ω --></mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo stretchy="false">(</mo>
              <mi>t</mi>
              <mo stretchy="false">)</mo>
            </mrow>
          </mfrac>
        </mrow>
        <mo>+</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mi>γ<!-- γ --></mi>
            <mi>K</mi>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p_{i}(t)=(1-\gamma ){\frac {\omega _{i}(t)}{\sum _{j=1}^{K}\omega _{j}(t)}}+{\frac {\gamma }{K}}}</annotation>
  </semantics>
</math></span><img alt="{\displaystyle p_{i}(t)=(1-\gamma ){\frac {\omega _{i}(t)}{\sum _{j=1}^{K}\omega _{j}(t)}}+{\frac {\gamma }{K}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/756c21a40b9c67c466703186086edc3ac00ddde6" style="vertical-align: -3.505ex; margin-left: -0.089ex; width:32.315ex; height:7.343ex;"/></span>       <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i=1,...,K}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mo>.</mo>
        <mo>.</mo>
        <mo>.</mo>
        <mo>,</mo>
        <mi>K</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle i=1,...,K}</annotation>
  </semantics>
</math></span><img alt="{\displaystyle i=1,...,K}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4fcf403a99f3f343092b1b23411804c0c0aa874c" style="vertical-align: -0.671ex; width:12.299ex; height:2.509ex;"/></span>
  2. Draw <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle i_{t}}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>i</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle i_{t}}</annotation>
  </semantics>
</math></span><img alt="i_{t}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6a9d68068946dcbba63e8d588198b5b8af2780f5" style="vertical-align: -0.671ex; width:1.628ex; height:2.509ex;"/></span> randomly according to the probabilities <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle p_{1}(t),...,p_{K}(t)}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo>,</mo>
        <mo>.</mo>
        <mo>.</mo>
        <mo>.</mo>
        <mo>,</mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>K</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p_{1}(t),...,p_{K}(t)}</annotation>
  </semantics>
</math></span><img alt="{\displaystyle p_{1}(t),...,p_{K}(t)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/93f7ea1bd2b366ff3e9dacd114519f46a04c2bdc" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:15.643ex; height:2.843ex;"/></span>
  3. Receive reward <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle x_{i_{t}}(t)\in [0,1]}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>i</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>t</mi>
              </mrow>
            </msub>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo>∈<!-- ∈ --></mo>
        <mo stretchy="false">[</mo>
        <mn>0</mn>
        <mo>,</mo>
        <mn>1</mn>
        <mo stretchy="false">]</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{i_{t}}(t)\in [0,1]}</annotation>
  </semantics>
</math></span><img alt="{\displaystyle x_{i_{t}}(t)\in [0,1]}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/10e3084214036b354d688f29dfdefb9cddf857a4" style="vertical-align: -0.838ex; width:12.918ex; height:2.843ex;"/></span>
  4. For <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle j=1,...,K}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>j</mi>
        <mo>=</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mo>.</mo>
        <mo>.</mo>
        <mo>.</mo>
        <mo>,</mo>
        <mi>K</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle j=1,...,K}</annotation>
  </semantics>
</math></span><img alt="{\displaystyle j=1,...,K}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5d0ca231321f3846f765a07d90b9f46d84a77f7a" style="vertical-align: -0.671ex; margin-left: -0.027ex; width:12.481ex; height:2.509ex;"/></span> set:
      <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle {\hat {x}}_{j}(t)={\begin{cases}x_{j}(t)/p_{j}(t)&amp;{\text{if }}j=i_{t}\\0,&amp;{\text{otherwise}}\end{cases}}}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mover>
                <mi>x</mi>
                <mo stretchy="false">^<!-- ^ --></mo>
              </mover>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow>
            <mo>{</mo>
            <mtable columnalign="left left" columnspacing="1em" displaystyle="false" rowspacing=".2em">
              <mtr>
                <mtd>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>j</mi>
                    </mrow>
                  </msub>
                  <mo stretchy="false">(</mo>
                  <mi>t</mi>
                  <mo stretchy="false">)</mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mo>/</mo>
                  </mrow>
                  <msub>
                    <mi>p</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>j</mi>
                    </mrow>
                  </msub>
                  <mo stretchy="false">(</mo>
                  <mi>t</mi>
                  <mo stretchy="false">)</mo>
                </mtd>
                <mtd>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>if </mtext>
                  </mrow>
                  <mi>j</mi>
                  <mo>=</mo>
                  <msub>
                    <mi>i</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>t</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>0</mn>
                  <mo>,</mo>
                </mtd>
                <mtd>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>otherwise</mtext>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
            <mo fence="true" stretchy="true" symmetric="true"></mo>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\hat {x}}_{j}(t)={\begin{cases}x_{j}(t)/p_{j}(t)&amp;{\text{if }}j=i_{t}\\0,&amp;{\text{otherwise}}\end{cases}}}</annotation>
  </semantics>
</math></span><img alt="{\displaystyle {\hat {x}}_{j}(t)={\begin{cases}x_{j}(t)/p_{j}(t)&amp;{\text{if }}j=i_{t}\\0,&amp;{\text{otherwise}}\end{cases}}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cb3212062c778b0acbd5b64ca1d9fb7a64c7752e" style="vertical-align: -2.505ex; width:33.16ex; height:6.176ex;"/></span>
 
      <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \omega _{j}(t+1)=\omega _{j}(t)exp(\gamma {\hat {x}}_{j}(t)/K)}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>ω<!-- ω --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo>+</mo>
        <mn>1</mn>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <msub>
          <mi>ω<!-- ω --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mi>e</mi>
        <mi>x</mi>
        <mi>p</mi>
        <mo stretchy="false">(</mo>
        <mi>γ<!-- γ --></mi>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mover>
                <mi>x</mi>
                <mo stretchy="false">^<!-- ^ --></mo>
              </mover>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <mi>K</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \omega _{j}(t+1)=\omega _{j}(t)exp(\gamma {\hat {x}}_{j}(t)/K)}</annotation>
  </semantics>
</math></span><img alt="{\displaystyle \omega _{j}(t+1)=\omega _{j}(t)exp(\gamma {\hat {x}}_{j}(t)/K)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9c7a6f0d34befaaa41abac445973d8fb29b9250f" style="vertical-align: -1.005ex; width:31.881ex; height:3.009ex;"/></span>
</pre>
<h5><span class="mw-headline" id="Explanation">Explanation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=21" title="Edit section: Explanation">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>Exp3 chooses an arm at random with probability <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle (1-\gamma )}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mo stretchy="false">(</mo>
<mn>1</mn>
<mo>−<!-- − --></mo>
<mi>γ<!-- γ --></mi>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle (1-\gamma )}</annotation>
</semantics>
</math></span><img alt="{\displaystyle (1-\gamma )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/82ee9d4dad4648965cadcab0a1f9230988bdb61b" style="vertical-align: -0.838ex; width:7.074ex; height:2.843ex;"/></span> it prefers arms with higher weights (exploit), it chooses with probability γ to uniformly randomly explore. After receiving the rewards the weights are updated. The exponential growth significantly increases the weight of good arms.
</p>
<h5><span class="mw-headline" id="Regret_analysis">Regret analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=22" title="Edit section: Regret analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>The (external) regret of the Exp3 algorithm is at most
<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle O({\sqrt {KTlog(K)}})}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>O</mi>
<mo stretchy="false">(</mo>
<mrow class="MJX-TeXAtom-ORD">
<msqrt>
<mi>K</mi>
<mi>T</mi>
<mi>l</mi>
<mi>o</mi>
<mi>g</mi>
<mo stretchy="false">(</mo>
<mi>K</mi>
<mo stretchy="false">)</mo>
</msqrt>
</mrow>
<mo stretchy="false">)</mo>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle O({\sqrt {KTlog(K)}})}</annotation>
</semantics>
</math></span><img alt="{\displaystyle O({\sqrt {KTlog(K)}})}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1b0065ce60939dfa33c3e35e6ea554cbb9d60f78" style="vertical-align: -1.838ex; width:16.421ex; height:4.843ex;"/></span>
</p>
<h4><span id="Follow_the_perturbed_leader_.28FPL.29_algorithm"></span><span class="mw-headline" id="Follow_the_perturbed_leader_(FPL)_algorithm">Follow the perturbed leader (FPL) algorithm</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=23" title="Edit section: Follow the perturbed leader (FPL) algorithm">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<h5><span class="mw-headline" id="Algorithm_2">Algorithm</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=24" title="Edit section: Algorithm">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<pre> <b>Parameters:</b> Real <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \eta }" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>η<!-- η --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta }</annotation>
  </semantics>
</math></span><img alt="\eta " aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e4d701857cf5fbec133eebaf94deadf722537f64" style="vertical-align: -0.838ex; width:1.169ex; height:2.176ex;"/></span>
 
 <b>Initialisation:</b> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \forall i:R_{i}(1)=0}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">∀<!-- ∀ --></mi>
        <mi>i</mi>
        <mo>:</mo>
        <msub>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \forall i:R_{i}(1)=0}</annotation>
  </semantics>
</math></span><img alt="{\displaystyle \forall i:R_{i}(1)=0}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6d7549d28e1139a2b9ad26aa438e60ac3cf00134" style="vertical-align: -0.838ex; width:13.829ex; height:2.843ex;"/></span>
 
 <b>For each</b> t = 1,2,...,T
  1. For each arm generate a random noise from an exponential distribution <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle \forall i:Z_{i}(t)\sim Exp(\eta )}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">∀<!-- ∀ --></mi>
        <mi>i</mi>
        <mo>:</mo>
        <msub>
          <mi>Z</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo>∼<!-- ∼ --></mo>
        <mi>E</mi>
        <mi>x</mi>
        <mi>p</mi>
        <mo stretchy="false">(</mo>
        <mi>η<!-- η --></mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \forall i:Z_{i}(t)\sim Exp(\eta )}</annotation>
  </semantics>
</math></span><img alt="{\displaystyle \forall i:Z_{i}(t)\sim Exp(\eta )}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c9eeb4c5a1ddf67f87ec2a7dd950295ef9259990" style="vertical-align: -0.838ex; width:19.42ex; height:2.843ex;"/></span>
  2. Pull arm <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle I(t)}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>I</mi>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle I(t)}</annotation>
  </semantics>
</math></span><img alt="I(t)" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e2434c9d80c34c95e25cc81ba6700f756a29dac5" style="vertical-align: -0.838ex; width:3.821ex; height:2.843ex;"/></span>: <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle I(t)=arg\max _{i}\{R_{i}(t)+Z_{i}(t)\}}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>I</mi>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>a</mi>
        <mi>r</mi>
        <mi>g</mi>
        <munder>
          <mo form="prefix" movablelimits="true">max</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </munder>
        <mo fence="false" stretchy="false">{</mo>
        <msub>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <msub>
          <mi>Z</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo fence="false" stretchy="false">}</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle I(t)=arg\max _{i}\{R_{i}(t)+Z_{i}(t)\}}</annotation>
  </semantics>
</math></span><img alt="{\displaystyle I(t)=arg\max _{i}\{R_{i}(t)+Z_{i}(t)\}}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5147a4eb45009e98a12f2697095a6745748975d1" style="vertical-align: -2.005ex; width:30.44ex; height:4.009ex;"/></span>
     Add noise to each arm and pull the one with the highest value
  3. Update value: <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle R_{I(t)}(t+1)=R_{I(t)}(t)+x_{I(t)}(t)}" xmlns="http://www.w3.org/1998/Math/MathML">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>I</mi>
            <mo stretchy="false">(</mo>
            <mi>t</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo>+</mo>
        <mn>1</mn>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <msub>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>I</mi>
            <mo stretchy="false">(</mo>
            <mi>t</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>I</mi>
            <mo stretchy="false">(</mo>
            <mi>t</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>t</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle R_{I(t)}(t+1)=R_{I(t)}(t)+x_{I(t)}(t)}</annotation>
  </semantics>
</math></span><img alt="{\displaystyle R_{I(t)}(t+1)=R_{I(t)}(t)+x_{I(t)}(t)}" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9798730b37449d804fe655d64f1e93d0d43bb5cc" style="vertical-align: -1.171ex; width:31.548ex; height:3.176ex;"/></span>
     The rest remains the same
</pre>
<h5><span class="mw-headline" id="Explanation_2">Explanation</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=25" title="Edit section: Explanation">edit</a><span class="mw-editsection-bracket">]</span></span></h5>
<p>We follow the arm that we think has the best performance so far adding exponential noise to it to provide exploration.<sup class="reference" id="cite_ref-44"><a href="#cite_note-44">[44]</a></sup>
</p>
<h4><span class="mw-headline" id="Exp3_vs_FPL">Exp3 vs FPL</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=26" title="Edit section: Exp3 vs FPL">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<table class="wikitable">
<tbody><tr>
<th>Exp3</th>
<th>FPL
</th></tr>
<tr>
<td>Maintains weights for each arm to calculate pulling probability</td>
<td>Doesn’t need to know the pulling probability per arm
</td></tr>
<tr>
<td>Has efficient theoretical guarantees</td>
<td>The standard FPL does not have good theoretical guarantees
</td></tr>
<tr>
<td>Might be computationally expensive (calculating the exponential terms)</td>
<td>Computationally quite efficient
</td></tr></tbody></table>
<h2><span class="mw-headline" id="Infinite-armed_bandit">Infinite-armed bandit</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=27" title="Edit section: Infinite-armed bandit">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In the original specification and in the above variants, the bandit problem is specified with a discrete and finite number of arms, often indicated by the variable <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle K}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>K</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle K}</annotation>
</semantics>
</math></span><img alt="K" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b76fce82a62ed5461908f0dc8f037de4e3686b0" style="vertical-align: -0.338ex; width:2.066ex; height:2.176ex;"/></span>. In the infinite armed case, introduced by Agarwal (1995), the "arms" are a continuous variable in <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math alttext="{\displaystyle K}" xmlns="http://www.w3.org/1998/Math/MathML">
<semantics>
<mrow class="MJX-TeXAtom-ORD">
<mstyle displaystyle="true" scriptlevel="0">
<mi>K</mi>
</mstyle>
</mrow>
<annotation encoding="application/x-tex">{\displaystyle K}</annotation>
</semantics>
</math></span><img alt="K" aria-hidden="true" class="mwe-math-fallback-image-inline" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b76fce82a62ed5461908f0dc8f037de4e3686b0" style="vertical-align: -0.338ex; width:2.066ex; height:2.176ex;"/></span> dimensions.
</p>
<h2><span class="mw-headline" id="Non-stationary_bandit">Non-stationary bandit</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=28" title="Edit section: Non-stationary bandit">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Garivier and Moulines derive some of the first results with respect to bandit problems where the underlying model can change during play. A number of algorithms were presented to deal with this case, including Discounted UCB<sup class="reference" id="cite_ref-45"><a href="#cite_note-45">[45]</a></sup> and Sliding-Window UCB.<sup class="reference" id="cite_ref-46"><a href="#cite_note-46">[46]</a></sup>
</p><p>Another work by Burtini et al. introduces a weighted least squares Thompson sampling approach (WLS-TS), which proves beneficial in both the known and unknown non-stationary cases <sup class="reference" id="cite_ref-47"><a href="#cite_note-47">[47]</a></sup>. In the known non-stationary case, the authors in <sup class="reference" id="cite_ref-AUCBDB_48-0"><a href="#cite_note-AUCBDB-48">[48]</a></sup> produce an alternative solution, a variant of UCB named Adjusted Upper Confidence Bound (A-UCB) which assumes a stochastic model and provide upper-bounds of the regret.
</p>
<h2><span class="mw-headline" id="Other_variants">Other variants</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=29" title="Edit section: Other variants">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Many variants of the problem have been proposed in recent years. 
</p>
<h3><span class="mw-headline" id="Dueling_bandit">Dueling bandit</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=30" title="Edit section: Dueling bandit">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The dueling bandit variant was introduced by Yue et al. (2012)<sup class="reference" id="cite_ref-YueEtAll2012_49-0"><a href="#cite_note-YueEtAll2012-49">[49]</a></sup> to model the exploration-versus-exploitation tradeoff for relative feedback.
In this variant the gambler is allowed to pull two levers at the same time, but they only get a binary feedback telling which lever provided the best reward. The difficulty of this problem stems from the fact that the gambler has no way of directly observing the reward of their actions.
The earliest algorithms for this problem are InterleaveFiltering,<sup class="reference" id="cite_ref-YueEtAll2012_49-1"><a href="#cite_note-YueEtAll2012-49">[49]</a></sup> Beat-The-Mean.<sup class="reference" id="cite_ref-Yue2011ICML:BTM_50-0"><a href="#cite_note-Yue2011ICML:BTM-50">[50]</a></sup>
The relative feedback of dueling bandits can also lead to <a class="mw-redirect" href="/wiki/Voting_paradoxes" title="Voting paradoxes">voting paradoxes</a>. A solution is to take the <a class="mw-redirect" href="/wiki/Condorcet_winner" title="Condorcet winner">Condorcet winner</a> as a reference.<sup class="reference" id="cite_ref-Urvoy2013ICML:SAVAGE_51-0"><a href="#cite_note-Urvoy2013ICML:SAVAGE-51">[51]</a></sup>
</p><p>More recently, researchers have generalized algorithms from traditional MAB to dueling bandits: Relative Upper Confidence Bounds (RUCB),<sup class="reference" id="cite_ref-Zoghi2014ICML:RUCB_52-0"><a href="#cite_note-Zoghi2014ICML:RUCB-52">[52]</a></sup> Relative EXponential weighing (REX3),<sup class="reference" id="cite_ref-Gajane2015ICML:REX3_53-0"><a href="#cite_note-Gajane2015ICML:REX3-53">[53]</a></sup> 
Copeland Confidence Bounds (CCB),<sup class="reference" id="cite_ref-Zoghi2015NIPS:CDB_54-0"><a href="#cite_note-Zoghi2015NIPS:CDB-54">[54]</a></sup> Relative Minimum Empirical Divergence (RMED),<sup class="reference" id="cite_ref-Komiyama2015COLT:DB_55-0"><a href="#cite_note-Komiyama2015COLT:DB-55">[55]</a></sup> and Double Thompson Sampling (DTS).<sup class="reference" id="cite_ref-Wu2016DTS_56-0"><a href="#cite_note-Wu2016DTS-56">[56]</a></sup>
</p>
<h3><span class="mw-headline" id="Collaborative_bandit">Collaborative bandit</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=31" title="Edit section: Collaborative bandit">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The collaborative filtering bandits (i.e., COFIBA) was introduced by Li and Karatzoglou and Gentile (SIGIR 2016),<sup class="reference" id="cite_ref-LKG2016COFIBA_57-0"><a href="#cite_note-LKG2016COFIBA-57">[57]</a></sup> where the classical collaborative filtering, and content-based filtering methods try to learn a static recommendation model given training data. These approaches are far from ideal in highly dynamic recommendation domains such as news recommendation and computational advertisement, where the set of items and users is very fluid. In this work, they investigate an adaptive clustering technique for content recommendation based on exploration-exploitation strategies in contextual multi-armed bandit settings.<sup class="reference" id="cite_ref-GLZ2014CLUB_58-0"><a href="#cite_note-GLZ2014CLUB-58">[58]</a></sup> Their algorithm (COFIBA, pronounced as "Coffee Bar") takes into account the collaborative effects<sup class="reference" id="cite_ref-LKG2016COFIBA_57-1"><a href="#cite_note-LKG2016COFIBA-57">[57]</a></sup> that arise due to the interaction of the users with the items, by dynamically grouping users based on the items under consideration and, at the same time, grouping items based on the similarity of the clusterings induced over the users. The resulting algorithm thus takes advantage of preference patterns in the data in a way akin to collaborative filtering methods. They provide an empirical analysis on medium-size real-world datasets, showing scalability and increased prediction performance (as measured by click-through rate) over state-of-the-art methods for clustering bandits. They also provide a regret analysis within a standard linear stochastic noise setting.
</p>
<h3><span class="mw-headline" id="Combinatorial_bandit">Combinatorial bandit</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=32" title="Edit section: Combinatorial bandit">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The Combinatorial Multiarmed Bandit (CMAB) problem<sup class="reference" id="cite_ref-gai2010learning_59-0"><a href="#cite_note-gai2010learning-59">[59]</a></sup><sup class="reference" id="cite_ref-chen2013combinatorial_60-0"><a href="#cite_note-chen2013combinatorial-60">[60]</a></sup><sup class="reference" id="cite_ref-ontanon2017combinatorial_61-0"><a href="#cite_note-ontanon2017combinatorial-61">[61]</a></sup> arises when instead of a single discrete variable to choose from, an agent needs to choose values for a set of variables. Assuming each variable is discrete, the number of possible choices per iteration is exponential in the number of variables. Several CMAB settings have been studied in the literature, from settings where the variables are binary<sup class="reference" id="cite_ref-chen2013combinatorial_60-1"><a href="#cite_note-chen2013combinatorial-60">[60]</a></sup> to more general setting where each variable can take an arbitrary set of values.<sup class="reference" id="cite_ref-ontanon2017combinatorial_61-1"><a href="#cite_note-ontanon2017combinatorial-61">[61]</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=33" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="/wiki/Gittins_index" title="Gittins index">Gittins index</a> – a powerful, general strategy for analyzing bandit problems.</li>
<li><a href="/wiki/Greedy_algorithm" title="Greedy algorithm">Greedy algorithm</a></li>
<li><a href="/wiki/Optimal_stopping" title="Optimal stopping">Optimal stopping</a></li>
<li><a href="/wiki/Search_theory" title="Search theory">Search theory</a></li>
<li><a href="/wiki/Stochastic_scheduling" title="Stochastic scheduling">Stochastic scheduling</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=34" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-doi10.1023/A:1013689704352-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-doi10.1023/A:1013689704352_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-doi10.1023/A:1013689704352_1-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Auer, P.; Cesa-Bianchi, N.; Fischer, P. (2002). "Finite-time Analysis of the Multiarmed Bandit Problem". <i>Machine Learning</i>. <b>47</b> (2/3): 235–256. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1023%2FA%3A1013689704352" rel="nofollow">10.1023/A:1013689704352</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Machine+Learning&amp;rft.atitle=Finite-time+Analysis+of+the+Multiarmed+Bandit+Problem&amp;rft.volume=47&amp;rft.issue=2%2F3&amp;rft.pages=235-256&amp;rft.date=2002&amp;rft_id=info%3Adoi%2F10.1023%2FA%3A1013689704352&amp;rft.aulast=Auer&amp;rft.aufirst=P.&amp;rft.au=Cesa-Bianchi%2C+N.&amp;rft.au=Fischer%2C+P.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><style data-mw-deduplicate="TemplateStyles:r886058088">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation journal">Katehakis, M. N.; Veinott, A. F. (1987). "The Multi-Armed Bandit Problem: Decomposition and Computation". <i>Mathematics of Operations Research</i>. <b>12</b> (2): 262–268. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1287%2Fmoor.12.2.262" rel="nofollow">10.1287/moor.12.2.262</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Mathematics+of+Operations+Research&amp;rft.atitle=The+Multi-Armed+Bandit+Problem%3A+Decomposition+and+Computation&amp;rft.volume=12&amp;rft.issue=2&amp;rft.pages=262-268&amp;rft.date=1987&amp;rft_id=info%3Adoi%2F10.1287%2Fmoor.12.2.262&amp;rft.aulast=Katehakis&amp;rft.aufirst=M.+N.&amp;rft.au=Veinott%2C+A.+F.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Gittins89-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-Gittins89_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Gittins89_3-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Gittins89_3-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-Gittins89_3-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text">
<cite class="citation" id="CITEREFGittins1989"><a href="/wiki/John_C._Gittins" title="John C. Gittins">Gittins, J. C.</a> (1989), <i>Multi-armed bandit allocation indices</i>, Wiley-Interscience Series in Systems and Optimization., Chichester: John Wiley &amp; Sons, Ltd., <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0-471-92059-5" title="Special:BookSources/978-0-471-92059-5"><bdi>978-0-471-92059-5</bdi></a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Multi-armed+bandit+allocation+indices&amp;rft.place=Chichester&amp;rft.series=Wiley-Interscience+Series+in+Systems+and+Optimization.&amp;rft.pub=John+Wiley+%26+Sons%2C+Ltd.&amp;rft.date=1989&amp;rft.isbn=978-0-471-92059-5&amp;rft.aulast=Gittins&amp;rft.aufirst=J.+C.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-BF-4"><span class="mw-cite-backlink">^ <a href="#cite_ref-BF_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-BF_4-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-BF_4-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-BF_4-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text">
<cite class="citation" id="CITEREFBerryFristedt1985"><a href="/wiki/Don_Berry_(statistician)" title="Don Berry (statistician)">Berry, Donald A.</a>; Fristedt, Bert (1985), <i>Bandit problems: Sequential allocation of experiments</i>, Monographs on Statistics and Applied Probability, London: Chapman &amp; Hall, <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0-412-24810-8" title="Special:BookSources/978-0-412-24810-8"><bdi>978-0-412-24810-8</bdi></a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Bandit+problems%3A+Sequential+allocation+of+experiments&amp;rft.place=London&amp;rft.series=Monographs+on+Statistics+and+Applied+Probability&amp;rft.pub=Chapman+%26+Hall&amp;rft.date=1985&amp;rft.isbn=978-0-412-24810-8&amp;rft.aulast=Berry&amp;rft.aufirst=Donald+A.&amp;rft.au=Fristedt%2C+Bert&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-weber-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-weber_5-0">^</a></b></span> <span class="reference-text"><cite class="citation" id="CITEREFWeber1992">Weber, Richard (1992), "On the Gittins index for multiarmed bandits", <i><a href="/wiki/Annals_of_Applied_Probability" title="Annals of Applied Probability">Annals of Applied Probability</a></i>, <b>2</b> (4): 1024–1033, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1214%2Faoap%2F1177005588" rel="nofollow">10.1214/aoap/1177005588</a>, <a href="/wiki/JSTOR" title="JSTOR">JSTOR</a> <a class="external text" href="//www.jstor.org/stable/2959678" rel="nofollow">2959678</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Applied+Probability&amp;rft.atitle=On+the+Gittins+index+for+multiarmed+bandits&amp;rft.volume=2&amp;rft.issue=4&amp;rft.pages=1024-1033&amp;rft.date=1992&amp;rft_id=info%3Adoi%2F10.1214%2Faoap%2F1177005588&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F2959678&amp;rft.aulast=Weber&amp;rft.aufirst=Richard&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation journal">Robbins, H. (1952). "Some aspects of the sequential design of experiments". <i>Bulletin of the American Mathematical Society</i>. <b>58</b> (5): 527–535. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1090%2FS0002-9904-1952-09620-8" rel="nofollow">10.1090/S0002-9904-1952-09620-8</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bulletin+of+the+American+Mathematical+Society&amp;rft.atitle=Some+aspects+of+the+sequential+design+of+experiments&amp;rft.volume=58&amp;rft.issue=5&amp;rft.pages=527-535&amp;rft.date=1952&amp;rft_id=info%3Adoi%2F10.1090%2FS0002-9904-1952-09620-8&amp;rft.aulast=Robbins&amp;rft.aufirst=H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/John_C._Gittins" title="John C. Gittins">J. C. Gittins</a> (1979). "Bandit Processes and Dynamic Allocation Indices". <i>Journal of the Royal Statistical Society. Series B (Methodological)</i>. <b>41</b> (2): 148–177. <a href="/wiki/JSTOR" title="JSTOR">JSTOR</a> <a class="external text" href="//www.jstor.org/stable/2985029" rel="nofollow">2985029</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+Royal+Statistical+Society.+Series+B+%28Methodological%29&amp;rft.atitle=Bandit+Processes+and+Dynamic+Allocation+Indices&amp;rft.volume=41&amp;rft.issue=2&amp;rft.pages=148-177&amp;rft.date=1979&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F2985029&amp;rft.au=J.+C.+Gittins&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-WHP-8"><span class="mw-cite-backlink">^ <a href="#cite_ref-WHP_8-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-WHP_8-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">
<cite class="citation" id="CITEREFPress2009">Press, William H. (2009), <a class="external text" href="http://www.pnas.org/content/106/52/22387" rel="nofollow">"Bandit solutions provide unified ethical models for randomized clinical trials and comparative effectiveness research"</a>, <i>Proceedings of the National Academy of Sciences</i>, <b>106</b> (52): 22387–22392, <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="http://adsabs.harvard.edu/abs/2009PNAS..10622387P" rel="nofollow">2009PNAS..10622387P</a>, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1073%2Fpnas.0912378106" rel="nofollow">10.1073/pnas.0912378106</a>, <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC2793317" rel="nofollow">2793317</a></span>, <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/20018711" rel="nofollow">20018711</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+National+Academy+of+Sciences&amp;rft.atitle=Bandit+solutions+provide+unified+ethical+models+for+randomized+clinical+trials+and+comparative+effectiveness+research&amp;rft.volume=106&amp;rft.issue=52&amp;rft.pages=22387-22392&amp;rft.date=2009&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC2793317&amp;rft_id=info%3Apmid%2F20018711&amp;rft_id=info%3Adoi%2F10.1073%2Fpnas.0912378106&amp;rft_id=info%3Abibcode%2F2009PNAS..10622387P&amp;rft.aulast=Press&amp;rft.aufirst=William+H.&amp;rft_id=http%3A%2F%2Fwww.pnas.org%2Fcontent%2F106%2F52%2F22387&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-KD-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-KD_9-0">^</a></b></span> <span class="reference-text">Press (1986)</span>
</li>
<li id="cite_note-BrochuHoffmandeFreitas-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-BrochuHoffmandeFreitas_10-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFBrochuHoffmande_Freitas2010">Brochu, Eric; Hoffman, Matthew W.; de Freitas, Nando (September 2010), <i>Portfolio Allocation for Bayesian Optimization</i>, <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1009.5419" rel="nofollow">1009.5419</a></span>, <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="http://adsabs.harvard.edu/abs/2010arXiv1009.5419B" rel="nofollow">2010arXiv1009.5419B</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Portfolio+Allocation+for+Bayesian+Optimization&amp;rft.date=2010-09&amp;rft_id=info%3Aarxiv%2F1009.5419&amp;rft_id=info%3Abibcode%2F2010arXiv1009.5419B&amp;rft.aulast=Brochu&amp;rft.aufirst=Eric&amp;rft.au=Hoffman%2C+Matthew+W.&amp;rft.au=de+Freitas%2C+Nando&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-ShenWangJiangZha-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-ShenWangJiangZha_11-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFShenWangJiangZha2015">Shen, Weiwei; Wang, Jun; Jiang, Yu-Gang; Zha, Hongyuan (2015), <a class="external text" href="http://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/viewPDFInterstitial/10972/10798" rel="nofollow">"Portfolio Choices with Orthogonal Bandit Learning"</a>, <i>Proceedings of International Joint Conferences on Artificial Intelligence (IJCAI2015)</i></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+International+Joint+Conferences+on+Artificial+Intelligence+%28IJCAI2015%29&amp;rft.atitle=Portfolio+Choices+with+Orthogonal+Bandit+Learning&amp;rft.date=2015&amp;rft.aulast=Shen&amp;rft.aufirst=Weiwei&amp;rft.au=Wang%2C+Jun&amp;rft.au=Jiang%2C+Yu-Gang&amp;rft.au=Zha%2C+Hongyuan&amp;rft_id=http%3A%2F%2Fwww.aaai.org%2Focs%2Findex.php%2FIJCAI%2FIJCAI15%2Fpaper%2FviewPDFInterstitial%2F10972%2F10798&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-farias2011irrevocable-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-farias2011irrevocable_12-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFFariasRitesh2011">Farias, Vivek F; Ritesh, Madan (2011), "The irrevocable multiarmed bandit problem", <i><a href="/wiki/Operations_Research_(journal)" title="Operations Research (journal)">Operations Research</a></i>, <b>59</b> (2): 383–399, <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.380.6983" rel="nofollow">10.1.1.380.6983</a></span>, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1287%2Fopre.1100.0891" rel="nofollow">10.1287/opre.1100.0891</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Operations+Research&amp;rft.atitle=The+irrevocable+multiarmed+bandit+problem&amp;rft.volume=59&amp;rft.issue=2&amp;rft.pages=383-399&amp;rft.date=2011&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.380.6983&amp;rft_id=info%3Adoi%2F10.1287%2Fopre.1100.0891&amp;rft.aulast=Farias&amp;rft.aufirst=Vivek+F&amp;rft.au=Ritesh%2C+Madan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Whittle79-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-Whittle79_13-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFWhittle1979"><a href="/wiki/Peter_Whittle_(mathematician)" title="Peter Whittle (mathematician)">Whittle, Peter</a> (1979), "Discussion of Dr Gittins' paper", <i><a href="/wiki/Journal_of_the_Royal_Statistical_Society" title="Journal of the Royal Statistical Society">Journal of the Royal Statistical Society</a></i>, Series B, <b>41</b> (2): 148–177, <a href="/wiki/JSTOR" title="JSTOR">JSTOR</a> <a class="external text" href="//www.jstor.org/stable/2985029" rel="nofollow">2985029</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+Royal+Statistical+Society&amp;rft.atitle=Discussion+of+Dr+Gittins%27+paper&amp;rft.volume=41&amp;rft.issue=2&amp;rft.pages=148-177&amp;rft.date=1979&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F2985029&amp;rft.aulast=Whittle&amp;rft.aufirst=Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Vermorel2005-14"><span class="mw-cite-backlink">^ <a href="#cite_ref-Vermorel2005_14-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Vermorel2005_14-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">
<cite class="citation" id="CITEREFVermorelMohri2005">Vermorel, Joannes; Mohri, Mehryar (2005), <a class="external text" href="http://bandit.sourceforge.net/Vermorel2005poker.pdf" rel="nofollow"><i>Multi-armed bandit algorithms and empirical evaluation</i></a> <span class="cs1-format">(PDF)</span>, In European Conference on Machine Learning, Springer, pp. 437–448</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Multi-armed+bandit+algorithms+and+empirical+evaluation&amp;rft.series=In+European+Conference+on+Machine+Learning&amp;rft.pages=437-448&amp;rft.pub=Springer&amp;rft.date=2005&amp;rft.aulast=Vermorel&amp;rft.aufirst=Joannes&amp;rft.au=Mohri%2C+Mehryar&amp;rft_id=http%3A%2F%2Fbandit.sourceforge.net%2FVermorel2005poker.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Whittle88-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-Whittle88_15-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFWhittle1988"><a href="/wiki/Peter_Whittle_(mathematician)" title="Peter Whittle (mathematician)">Whittle, Peter</a> (1988), "Restless bandits: Activity allocation in a changing world", <i>Journal of Applied Probability</i>, <b>25A</b>: 287–298, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.2307%2F3214163" rel="nofollow">10.2307/3214163</a>, <a href="/wiki/JSTOR" title="JSTOR">JSTOR</a> <a class="external text" href="//www.jstor.org/stable/3214163" rel="nofollow">3214163</a>, <a href="/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a> <a class="external text" href="//www.ams.org/mathscinet-getitem?mr=0974588" rel="nofollow">0974588</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Applied+Probability&amp;rft.atitle=Restless+bandits%3A+Activity+allocation+in+a+changing+world&amp;rft.volume=25A&amp;rft.pages=287-298&amp;rft.date=1988&amp;rft_id=%2F%2Fwww.ams.org%2Fmathscinet-getitem%3Fmr%3D974588&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F3214163&amp;rft_id=info%3Adoi%2F10.2307%2F3214163&amp;rft.aulast=Whittle&amp;rft.aufirst=Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Whittle81-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-Whittle81_16-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFWhittle1981"><a href="/wiki/Peter_Whittle_(mathematician)" title="Peter Whittle (mathematician)">Whittle, Peter</a> (1981), "Arm-acquiring bandits", <i>Annals of Probability</i>, <b>9</b> (2): 284–292, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1214%2Faop%2F1176994469" rel="nofollow">10.1214/aop/1176994469</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Probability&amp;rft.atitle=Arm-acquiring+bandits&amp;rft.volume=9&amp;rft.issue=2&amp;rft.pages=284-292&amp;rft.date=1981&amp;rft_id=info%3Adoi%2F10.1214%2Faop%2F1176994469&amp;rft.aulast=Whittle&amp;rft.aufirst=Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><cite class="citation journal">Auer, P.; Cesa-Bianchi, N.; Freund, Y.; Schapire, R. E. (2002). "The Nonstochastic Multiarmed Bandit Problem". <i><a href="/wiki/SIAM_Journal_on_Computing" title="SIAM Journal on Computing">SIAM J. Comput.</a></i> <b>32</b> (1): 48–77. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.130.158" rel="nofollow">10.1.1.130.158</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1137%2FS0097539701398375" rel="nofollow">10.1137/S0097539701398375</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SIAM+J.+Comput.&amp;rft.atitle=The+Nonstochastic+Multiarmed+Bandit+Problem&amp;rft.volume=32&amp;rft.issue=1&amp;rft.pages=48-77&amp;rft.date=2002&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.130.158&amp;rft_id=info%3Adoi%2F10.1137%2FS0097539701398375&amp;rft.aulast=Auer&amp;rft.aufirst=P.&amp;rft.au=Cesa-Bianchi%2C+N.&amp;rft.au=Freund%2C+Y.&amp;rft.au=Schapire%2C+R.+E.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><cite class="citation journal">Lai, T.L.; Robbins, H. (1985). "Asymptotically efficient adaptive allocation rules". <i>Advances in Applied Mathematics</i>. <b>6</b> (1): 4–22. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1016%2F0196-8858%2885%2990002-8" rel="nofollow">10.1016/0196-8858(85)90002-8</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Applied+Mathematics&amp;rft.atitle=Asymptotically+efficient+adaptive+allocation+rules&amp;rft.volume=6&amp;rft.issue=1&amp;rft.pages=4-22&amp;rft.date=1985&amp;rft_id=info%3Adoi%2F10.1016%2F0196-8858%2885%2990002-8&amp;rft.aulast=Lai&amp;rft.aufirst=T.L.&amp;rft.au=Robbins%2C+H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><cite class="citation journal">Katehakis, M.N.; Robbins, H. (1995). <a class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC41010" rel="nofollow">"Sequential choice from several populations"</a>. <i>Proceedings of the National Academy of Sciences of the United States of America</i>. <b>92</b> (19): 8584–5. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="http://adsabs.harvard.edu/abs/1995PNAS...92.8584K" rel="nofollow">1995PNAS...92.8584K</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1073%2Fpnas.92.19.8584" rel="nofollow">10.1073/pnas.92.19.8584</a>. <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC41010" rel="nofollow">41010</a></span>. <a class="mw-redirect" href="/wiki/PubMed_Identifier" title="PubMed Identifier">PMID</a> <a class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/11607577" rel="nofollow">11607577</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+National+Academy+of+Sciences+of+the+United+States+of+America&amp;rft.atitle=Sequential+choice+from+several+populations&amp;rft.volume=92&amp;rft.issue=19&amp;rft.pages=8584-5&amp;rft.date=1995&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC41010&amp;rft_id=info%3Apmid%2F11607577&amp;rft_id=info%3Adoi%2F10.1073%2Fpnas.92.19.8584&amp;rft_id=info%3Abibcode%2F1995PNAS...92.8584K&amp;rft.aulast=Katehakis&amp;rft.aufirst=M.N.&amp;rft.au=Robbins%2C+H.&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC41010&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><cite class="citation journal">Burnetas, A.N.; Katehakis, M.N. (1996). "Optimal adaptive policies for sequential allocation problems". <i>Advances in Applied Mathematics</i>. <b>17</b> (2): 122–142. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1006%2Faama.1996.0007" rel="nofollow">10.1006/aama.1996.0007</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Applied+Mathematics&amp;rft.atitle=Optimal+adaptive+policies+for+sequential+allocation+problems&amp;rft.volume=17&amp;rft.issue=2&amp;rft.pages=122-142&amp;rft.date=1996&amp;rft_id=info%3Adoi%2F10.1006%2Faama.1996.0007&amp;rft.aulast=Burnetas&amp;rft.aufirst=A.N.&amp;rft.au=Katehakis%2C+M.N.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><cite class="citation journal">Burnetas, A.N.; Katehakis, M.N. (1997). "Optimal adaptive policies for Markov decision processes". <i>Math. Oper. Res</i>. <b>22</b> (1): 222–255. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1287%2Fmoor.22.1.222" rel="nofollow">10.1287/moor.22.1.222</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Math.+Oper.+Res.&amp;rft.atitle=Optimal+adaptive+policies+for+Markov+decision+processes&amp;rft.volume=22&amp;rft.issue=1&amp;rft.pages=222-255&amp;rft.date=1997&amp;rft_id=info%3Adoi%2F10.1287%2Fmoor.22.1.222&amp;rft.aulast=Burnetas&amp;rft.aufirst=A.N.&amp;rft.au=Katehakis%2C+M.N.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><cite class="citation journal">Tewari, A.; Bartlett, P.L. (2008). <a class="external text" href="http://books.nips.cc/papers/files/nips20/NIPS2007_0673.pdf" rel="nofollow">"Optimistic linear programming gives logarithmic regret for irreducible MDPs"</a> <span class="cs1-format">(PDF)</span>. <i>Advances in Neural Information Processing Systems</i>. <b>20</b>. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.69.5482" rel="nofollow">10.1.1.69.5482</a></span>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.atitle=Optimistic+linear+programming+gives+logarithmic+regret+for+irreducible+MDPs&amp;rft.volume=20&amp;rft.date=2008&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.69.5482&amp;rft.aulast=Tewari&amp;rft.aufirst=A.&amp;rft.au=Bartlett%2C+P.L.&amp;rft_id=http%3A%2F%2Fbooks.nips.cc%2Fpapers%2Ffiles%2Fnips20%2FNIPS2007_0673.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><cite class="citation journal">Ortner, R. (2010). "Online regret bounds for Markov decision processes with deterministic transitions". <i>Theoretical Computer Science</i>. <b>411</b> (29): 2684–2695. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1016%2Fj.tcs.2010.04.005" rel="nofollow">10.1016/j.tcs.2010.04.005</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Theoretical+Computer+Science&amp;rft.atitle=Online+regret+bounds+for+Markov+decision+processes+with+deterministic+transitions&amp;rft.volume=411&amp;rft.issue=29&amp;rft.pages=2684-2695&amp;rft.date=2010&amp;rft_id=info%3Adoi%2F10.1016%2Fj.tcs.2010.04.005&amp;rft.aulast=Ortner&amp;rft.aufirst=R.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text">Filippi, S. and Cappé, O. and Garivier, A. (2010). "Online regret bounds for Markov decision processes with deterministic transitions", <i>Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on</i>, pp. 115–122</span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text"><cite class="citation journal">Honda, J.; Takemura, A. (2011). "An asymptotically optimal policy for finite support models in the multi-armed bandit problem". <i>Machine Learning</i>. <b>85</b> (3): 361–391. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/0905.2776" rel="nofollow">0905.2776</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1007%2Fs10994-011-5257-4" rel="nofollow">10.1007/s10994-011-5257-4</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Machine+Learning&amp;rft.atitle=An+asymptotically+optimal+policy+for+finite+support+models+in+the+multi-armed+bandit+problem&amp;rft.volume=85&amp;rft.issue=3&amp;rft.pages=361-391&amp;rft.date=2011&amp;rft_id=info%3Aarxiv%2F0905.2776&amp;rft_id=info%3Adoi%2F10.1007%2Fs10994-011-5257-4&amp;rft.aulast=Honda&amp;rft.aufirst=J.&amp;rft.au=Takemura%2C+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text">Sutton, R. S. &amp; Barto, A. G. 1998 Reinforcement learning: an introduction. Cambridge, MA: MIT Press.</span>
</li>
<li id="cite_note-Tokic2010-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-Tokic2010_27-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFTokic2010">Tokic, Michel (2010), <a class="external text" href="http://www.tokic.com/www/tokicm/publikationen/papers/AdaptiveEpsilonGreedyExploration.pdf" rel="nofollow">"Adaptive ε-greedy exploration in reinforcement learning based on value differences"</a> <span class="cs1-format">(PDF)</span>, <i>KI 2010: Advances in Artificial Intelligence</i>, Lecture Notes in Computer Science, <b>6359</b>, Springer-Verlag, pp. 203–210, <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.458.464" rel="nofollow">10.1.1.458.464</a></span>, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1007%2F978-3-642-16111-7_23" rel="nofollow">10.1007/978-3-642-16111-7_23</a>, <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-3-642-16110-0" title="Special:BookSources/978-3-642-16110-0"><bdi>978-3-642-16110-0</bdi></a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Adaptive+%CE%B5-greedy+exploration+in+reinforcement+learning+based+on+value+differences&amp;rft.btitle=KI+2010%3A+Advances+in+Artificial+Intelligence&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=203-210&amp;rft.pub=Springer-Verlag&amp;rft.date=2010&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.458.464&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-16111-7_23&amp;rft.isbn=978-3-642-16110-0&amp;rft.aulast=Tokic&amp;rft.aufirst=Michel&amp;rft_id=http%3A%2F%2Fwww.tokic.com%2Fwww%2Ftokicm%2Fpublikationen%2Fpapers%2FAdaptiveEpsilonGreedyExploration.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/>.</span>
</li>
<li id="cite_note-TokicPalm2011-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-TokicPalm2011_28-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFTokicPalm2011">Tokic, Michel; Palm, Günther (2011), <a class="external text" href="http://www.tokic.com/www/tokicm/publikationen/papers/KI2011.pdf" rel="nofollow">"Value-Difference Based Exploration: Adaptive Control Between Epsilon-Greedy and Softmax"</a> <span class="cs1-format">(PDF)</span>, <i>KI 2011: Advances in Artificial Intelligence</i>, Lecture Notes in Computer Science, <b>7006</b>, Springer-Verlag, pp. 335–346, <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-3-642-24455-1" title="Special:BookSources/978-3-642-24455-1"><bdi>978-3-642-24455-1</bdi></a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Value-Difference+Based+Exploration%3A+Adaptive+Control+Between+Epsilon-Greedy+and+Softmax&amp;rft.btitle=KI+2011%3A+Advances+in+Artificial+Intelligence&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=335-346&amp;rft.pub=Springer-Verlag&amp;rft.date=2011&amp;rft.isbn=978-3-642-24455-1&amp;rft.aulast=Tokic&amp;rft.aufirst=Michel&amp;rft.au=Palm%2C+G%C3%BCnther&amp;rft_id=http%3A%2F%2Fwww.tokic.com%2Fwww%2Ftokicm%2Fpublikationen%2Fpapers%2FKI2011.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/>.</span>
</li>
<li id="cite_note-Bouneffouf2012-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-Bouneffouf2012_29-0">^</a></b></span> <span class="reference-text">
<cite class="citation book">Bouneffouf, D.; Bouzeghoub, A.; Gançarski, A. L. (2012). "A Contextual-Bandit Algorithm for Mobile Context-Aware Recommender System". <i>Neural Information Processing</i>. Lecture Notes in Computer Science. <b>7665</b>. p. 324. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1007%2F978-3-642-34487-9_40" rel="nofollow">10.1007/978-3-642-34487-9_40</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-3-642-34486-2" title="Special:BookSources/978-3-642-34486-2"><bdi>978-3-642-34486-2</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=A+Contextual-Bandit+Algorithm+for+Mobile+Context-Aware+Recommender+System&amp;rft.btitle=Neural+Information+Processing&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=324&amp;rft.date=2012&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-34487-9_40&amp;rft.isbn=978-3-642-34486-2&amp;rft.aulast=Bouneffouf&amp;rft.aufirst=D.&amp;rft.au=Bouzeghoub%2C+A.&amp;rft.au=Gan%C3%A7arski%2C+A.+L.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Scott2010-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-Scott2010_30-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFScott2010">Scott, S.L. (2010), "A modern Bayesian look at the multi-armed bandit", <i>Applied Stochastic Models in Business and Industry</i>, <b>26</b> (2): 639–658, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1002%2Fasmb.874" rel="nofollow">10.1002/asmb.874</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Applied+Stochastic+Models+in+Business+and+Industry&amp;rft.atitle=A+modern+Bayesian+look+at+the+multi-armed+bandit&amp;rft.volume=26&amp;rft.issue=2&amp;rft.pages=639-658&amp;rft.date=2010&amp;rft_id=info%3Adoi%2F10.1002%2Fasmb.874&amp;rft.aulast=Scott&amp;rft.aufirst=S.L.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Langford2008-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-Langford2008_31-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFLangfordZhang2008">Langford, John; Zhang, Tong (2008), <a class="external text" href="http://papers.nips.cc/paper/3178-the-epoch-greedy-algorithm-for-multi-armed-bandits-with-side-information" rel="nofollow">"The Epoch-Greedy Algorithm for Contextual Multi-armed Bandits"</a>, <i>Advances in Neural Information Processing Systems 20</i>, Curran Associates, Inc., pp. 817–824</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=The+Epoch-Greedy+Algorithm+for+Contextual+Multi-armed+Bandits&amp;rft.btitle=Advances+in+Neural+Information+Processing+Systems+20&amp;rft.pages=817-824&amp;rft.pub=Curran+Associates%2C+Inc.&amp;rft.date=2008&amp;rft.aulast=Langford&amp;rft.aufirst=John&amp;rft.au=Zhang%2C+Tong&amp;rft_id=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F3178-the-epoch-greedy-algorithm-for-multi-armed-bandits-with-side-information&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><cite class="citation book">Auer, P. (2000). <i>Using upper confidence bounds for online learning</i>. <i>Proceedings 41st Annual Symposium on Foundations of Computer Science</i>. IEEE Comput. Soc. pp. 270–279. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1109%2Fsfcs.2000.892116" rel="nofollow">10.1109/sfcs.2000.892116</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0769508504" title="Special:BookSources/978-0769508504"><bdi>978-0769508504</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Using+upper+confidence+bounds+for+online+learning&amp;rft.pages=270-279&amp;rft.pub=IEEE+Comput.+Soc&amp;rft.date=2000&amp;rft_id=info%3Adoi%2F10.1109%2Fsfcs.2000.892116&amp;rft.isbn=978-0769508504&amp;rft.aulast=Auer&amp;rft.aufirst=P.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text"><cite class="citation book">Hong, Tzung-Pei; Song, Wei-Ping; Chiu, Chu-Tien (November 2011). <i>Evolutionary Composite Attribute Clustering</i>. <i>2011 International Conference on Technologies and Applications of Artificial Intelligence</i>. IEEE. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1109%2Ftaai.2011.59" rel="nofollow">10.1109/taai.2011.59</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/9781457721748" title="Special:BookSources/9781457721748"><bdi>9781457721748</bdi></a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Evolutionary+Composite+Attribute+Clustering&amp;rft.pub=IEEE&amp;rft.date=2011-11&amp;rft_id=info%3Adoi%2F10.1109%2Ftaai.2011.59&amp;rft.isbn=9781457721748&amp;rft.aulast=Hong&amp;rft.aufirst=Tzung-Pei&amp;rft.au=Song%2C+Wei-Ping&amp;rft.au=Chiu%2C+Chu-Tien&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-RigZee10-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-RigZee10_34-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFRigolletZeevi2010">Rigollet, Philippe; Zeevi, Assaf (2010), <i>Nonparametric Bandits with Covariates</i>, Conference on Learning Theory, COLT 2010</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Nonparametric+Bandits+with+Covariates&amp;rft.series=Conference+on+Learning+Theory%2C+COLT+2010&amp;rft.date=2010&amp;rft.aulast=Rigollet&amp;rft.aufirst=Philippe&amp;rft.au=Zeevi%2C+Assaf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-slivkins11-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-slivkins11_35-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFSlivkins2011">Slivkins, Aleksandrs (2011), <i>Contextual bandits with similarity information.</i>, Conference on Learning Theory, COLT 2011</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Contextual+bandits+with+similarity+information.&amp;rft.series=Conference+on+Learning+Theory%2C+COLT+2011&amp;rft.date=2011&amp;rft.aulast=Slivkins&amp;rft.aufirst=Aleksandrs&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-PerRig13-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-PerRig13_36-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFPerchetRigollet2013">Perchet, Vianney; Rigollet, Philippe (2013), "The multi-armed bandit problem with covariates", <i><a href="/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>, <b>41</b> (2): 693–721, <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1110.6084" rel="nofollow">1110.6084</a></span>, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1214%2F13-aos1101" rel="nofollow">10.1214/13-aos1101</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Statistics&amp;rft.atitle=The+multi-armed+bandit+problem+with+covariates&amp;rft.volume=41&amp;rft.issue=2&amp;rft.pages=693-721&amp;rft.date=2013&amp;rft_id=info%3Aarxiv%2F1110.6084&amp;rft_id=info%3Adoi%2F10.1214%2F13-aos1101&amp;rft.aulast=Perchet&amp;rft.aufirst=Vianney&amp;rft.au=Rigollet%2C+Philippe&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Robin2014-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-Robin2014_37-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFAllesiardoFéraudDjallel2014">Allesiardo, Robin; Féraud, Raphaël; Djallel, Bouneffouf (2014), "A Neural Networks Committee for the Contextual Bandit Problem", <i>Neural Information Processing – 21st International Conference, ICONIP 2014, Malaisia, November 03-06,2014, Proceedings</i>, <a href="/wiki/Lecture_Notes_in_Computer_Science" title="Lecture Notes in Computer Science">Lecture Notes in Computer Science</a>, <b>8834</b>, Springer, pp. 374–381, <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1409.8191" rel="nofollow">1409.8191</a></span>, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1007%2F978-3-319-12637-1_47" rel="nofollow">10.1007/978-3-319-12637-1_47</a>, <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-3-319-12636-4" title="Special:BookSources/978-3-319-12636-4"><bdi>978-3-319-12636-4</bdi></a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=A+Neural+Networks+Committee+for+the+Contextual+Bandit+Problem&amp;rft.btitle=Neural+Information+Processing+%E2%80%93+21st+International+Conference%2C+ICONIP+2014%2C+Malaisia%2C+November+03-06%2C2014%2C+Proceedings&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=374-381&amp;rft.pub=Springer&amp;rft.date=2014&amp;rft_id=info%3Aarxiv%2F1409.8191&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-319-12637-1_47&amp;rft.isbn=978-3-319-12636-4&amp;rft.aulast=Allesiardo&amp;rft.aufirst=Robin&amp;rft.au=F%C3%A9raud%2C+Rapha%C3%ABl&amp;rft.au=Djallel%2C+Bouneffouf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Valko2014-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-Valko2014_38-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFMichal_ValkoNathan_KordaRémi_MunosIlias_Flaounas2013">Michal Valko; Nathan Korda; Rémi Munos; Ilias Flaounas; Nello Cristianini (2013), <i>Finite-Time Analysis of Kernelised Contextual Bandits</i>, 29th Conference on Uncertainty in Artificial Intelligence (UAI 2013) and (JFPDA 2013)., <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1309.6869" rel="nofollow">1309.6869</a></span>, <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="http://adsabs.harvard.edu/abs/2013arXiv1309.6869V" rel="nofollow">2013arXiv1309.6869V</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Finite-Time+Analysis+of+Kernelised+Contextual+Bandits&amp;rft.series=29th+Conference+on+Uncertainty+in+Artificial+Intelligence+%28UAI+2013%29+and+%28JFPDA+2013%29.&amp;rft.date=2013&amp;rft_id=info%3Aarxiv%2F1309.6869&amp;rft_id=info%3Abibcode%2F2013arXiv1309.6869V&amp;rft.au=Michal+Valko&amp;rft.au=Nathan+Korda&amp;rft.au=R%C3%A9mi+Munos&amp;rft.au=Ilias+Flaounas&amp;rft.au=Nello+Cristianini&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-39">^</a></b></span> <span class="reference-text"><cite class="citation journal">Féraud, Raphaël; Allesiardo, Robin; Urvoy, Tanguy; Clérot, Fabrice (2016). <a class="external text" href="http://jmlr.org/proceedings/papers/v51/feraud16.html" rel="nofollow">"Random Forest for the Contextual Bandit Problem"</a>. <i>Aistats</i>: 93–101.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Aistats&amp;rft.atitle=Random+Forest+for+the+Contextual+Bandit+Problem&amp;rft.pages=93-101&amp;rft.date=2016&amp;rft.aulast=F%C3%A9raud&amp;rft.aufirst=Rapha%C3%ABl&amp;rft.au=Allesiardo%2C+Robin&amp;rft.au=Urvoy%2C+Tanguy&amp;rft.au=Cl%C3%A9rot%2C+Fabrice&amp;rft_id=http%3A%2F%2Fjmlr.org%2Fproceedings%2Fpapers%2Fv51%2Fferaud16.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Badanidiyuru2014COLT-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-Badanidiyuru2014COLT_40-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFBadanidiyuruLangfordSlivkins2014">Badanidiyuru, A.; Langford, J.; Slivkins, A. (2014), "Resourceful contextual bandits", <i>Proceeding of Conference on Learning Theory (COLT)</i></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Resourceful+contextual+bandits&amp;rft.btitle=Proceeding+of+Conference+on+Learning+Theory+%28COLT%29&amp;rft.date=2014&amp;rft.aulast=Badanidiyuru&amp;rft.aufirst=A.&amp;rft.au=Langford%2C+J.&amp;rft.au=Slivkins%2C+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Wu2015UCBALP-41"><span class="mw-cite-backlink">^ <a href="#cite_ref-Wu2015UCBALP_41-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Wu2015UCBALP_41-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">
<cite class="citation" id="CITEREFWuSrikantLiuJiang2015">Wu, Huasen; Srikant, R.; Liu, Xin; Jiang, Chong (2015), <a class="external text" href="https://papers.nips.cc/paper/6008-algorithms-with-logarithmic-or-sublinear-regret-for-constrained-contextual-bandits" rel="nofollow">"Algorithms with Logarithmic or Sublinear Regret for Constrained Contextual Bandits"</a>, <i>The 29th Annual Conference on Neural Information Processing Systems (NIPS)</i>: 433–441</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+29th+Annual+Conference+on+Neural+Information+Processing+Systems+%28NIPS%29&amp;rft.atitle=Algorithms+with+Logarithmic+or+Sublinear+Regret+for+Constrained+Contextual+Bandits&amp;rft.pages=433-441&amp;rft.date=2015&amp;rft.aulast=Wu&amp;rft.aufirst=Huasen&amp;rft.au=Srikant%2C+R.&amp;rft.au=Liu%2C+Xin&amp;rft.au=Jiang%2C+Chong&amp;rft_id=https%3A%2F%2Fpapers.nips.cc%2Fpaper%2F6008-algorithms-with-logarithmic-or-sublinear-regret-for-constrained-contextual-bandits&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-42"><span class="mw-cite-backlink"><b><a href="#cite_ref-42">^</a></b></span> <span class="reference-text">Burtini, Giuseppe, Jason Loeppky, and Ramon Lawrence. "A survey of online experiment design with the stochastic multi-armed bandit." arXiv preprint arXiv:1510.00757 (2015).</span>
</li>
<li id="cite_note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43">^</a></b></span> <span class="reference-text">Seldin, Y., Szepesvári, C., Auer, P. and Abbasi-Yadkori, Y., 2012, December. Evaluation and Analysis of the Performance of the EXP3 Algorithm in Stochastic Environments. In EWRL (pp. 103–116).</span>
</li>
<li id="cite_note-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-44">^</a></b></span> <span class="reference-text">Hutter, M. and Poland, J., 2005. Adaptive online prediction by following the perturbed leader. Journal of Machine Learning Research, 6(Apr), pp.639–660.</span>
</li>
<li id="cite_note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45">^</a></b></span> <span class="reference-text">Discounted UCB, Levente Kocsis, Csaba Szepesvári, 2006</span>
</li>
<li id="cite_note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text">On Upper-Confidence Bound Policies for Non-Stationary Bandit Problems, Garivier and Moulines, 2008 &lt;<a class="external free" href="https://arxiv.org/abs/0805.3415" rel="nofollow">https://arxiv.org/abs/0805.3415</a>&gt;</span>
</li>
<li id="cite_note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text">Improving Online Marketing Experiments with Drifting Multi-armed Bandits, Giuseppe Burtini, Jason Loeppky, Ramon Lawrence, 2015 &lt;<a class="external free" href="http://www.scitepress.org/DigitalLibrary/PublicationsDetail.aspx?ID=Dx2xXEB0PJE=&amp;t=1" rel="nofollow">http://www.scitepress.org/DigitalLibrary/PublicationsDetail.aspx?ID=Dx2xXEB0PJE=&amp;t=1</a>&gt;</span>
</li>
<li id="cite_note-AUCBDB-48"><span class="mw-cite-backlink"><b><a href="#cite_ref-AUCBDB_48-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFBouneffoufFeraud2016">Bouneffouf, Djallel; Feraud, Raphael (2016), "Multi-armed bandit problem with known trend", <i>Neurocomputing</i></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Multi-armed+bandit+problem+with+known+trend&amp;rft.btitle=Neurocomputing&amp;rft.date=2016&amp;rft.aulast=Bouneffouf&amp;rft.aufirst=Djallel&amp;rft.au=Feraud%2C+Raphael&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-YueEtAll2012-49"><span class="mw-cite-backlink">^ <a href="#cite_ref-YueEtAll2012_49-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-YueEtAll2012_49-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">
<cite class="citation" id="CITEREFYueBroderKleinbergJoachims2012">Yue, Yisong; Broder, Josef; Kleinberg, Robert; Joachims, Thorsten (2012), "Journal of Computer and System Sciences", <i>Journal of Computer and System Sciences</i>, <b>78</b> (5): 1538–1556, <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a> <span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.162.2764" rel="nofollow">10.1.1.162.2764</a></span>, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1016%2Fj.jcss.2011.12.028" rel="nofollow">10.1016/j.jcss.2011.12.028</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Computer+and+System+Sciences&amp;rft.atitle=Journal+of+Computer+and+System+Sciences&amp;rft.volume=78&amp;rft.issue=5&amp;rft.pages=1538-1556&amp;rft.date=2012&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.162.2764&amp;rft_id=info%3Adoi%2F10.1016%2Fj.jcss.2011.12.028&amp;rft.aulast=Yue&amp;rft.aufirst=Yisong&amp;rft.au=Broder%2C+Josef&amp;rft.au=Kleinberg%2C+Robert&amp;rft.au=Joachims%2C+Thorsten&amp;rft_id=http%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2FS0022000012000281&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span> <span class="cs1-visible-error error citation-comment"><code class="cs1-code">|chapter=</code> ignored (<a href="/wiki/Help:CS1_errors#chapter_ignored" title="Help:CS1 errors">help</a>)</span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Yue2011ICML:BTM-50"><span class="mw-cite-backlink"><b><a href="#cite_ref-Yue2011ICML:BTM_50-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFYueJoachims2011">Yue, Yisong; Joachims, Thorsten (2011), "Beat the Mean Bandit", <i>Proceedings of ICML'11</i></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Beat+the+Mean+Bandit&amp;rft.btitle=Proceedings+of+ICML%2711&amp;rft.date=2011&amp;rft.aulast=Yue&amp;rft.aufirst=Yisong&amp;rft.au=Joachims%2C+Thorsten&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Urvoy2013ICML:SAVAGE-51"><span class="mw-cite-backlink"><b><a href="#cite_ref-Urvoy2013ICML:SAVAGE_51-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFUrvoyClérotFéraudNaamane2013">Urvoy, Tanguy; Clérot, Fabrice; Féraud, Raphaël; Naamane, Sami (2013), <a class="external text" href="http://www.jmlr.org/proceedings/papers/v28/urvoy13.pdf" rel="nofollow">"Generic Exploration and K-armed Voting Bandits"</a> <span class="cs1-format">(PDF)</span>, <i>Proceedings of the 30th International Conference on Machine Learning (ICML-13)</i></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Generic+Exploration+and+K-armed+Voting+Bandits&amp;rft.btitle=Proceedings+of+the+30th+International+Conference+on+Machine+Learning+%28ICML-13%29&amp;rft.date=2013&amp;rft.aulast=Urvoy&amp;rft.aufirst=Tanguy&amp;rft.au=Cl%C3%A9rot%2C+Fabrice&amp;rft.au=F%C3%A9raud%2C+Rapha%C3%ABl&amp;rft.au=Naamane%2C+Sami&amp;rft_id=http%3A%2F%2Fwww.jmlr.org%2Fproceedings%2Fpapers%2Fv28%2Furvoy13.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Zoghi2014ICML:RUCB-52"><span class="mw-cite-backlink"><b><a href="#cite_ref-Zoghi2014ICML:RUCB_52-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFZoghiWhitesonMunosRijke2014">Zoghi, Masrour; Whiteson, Shimon; Munos, Remi; Rijke, Maarten D (2014), <a class="external text" href="http://www.jmlr.org/proceedings/papers/v32/zoghi14.pdf" rel="nofollow">"Relative Upper Confidence Bound for the $K$-Armed Dueling Bandit Problem"</a> <span class="cs1-format">(PDF)</span>, <i>Proceedings of the 31st International Conference on Machine Learning (ICML-14)</i></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Relative+Upper+Confidence+Bound+for+the+%24K%24-Armed+Dueling+Bandit+Problem&amp;rft.btitle=Proceedings+of+the+31st+International+Conference+on+Machine+Learning+%28ICML-14%29&amp;rft.date=2014&amp;rft.aulast=Zoghi&amp;rft.aufirst=Masrour&amp;rft.au=Whiteson%2C+Shimon&amp;rft.au=Munos%2C+Remi&amp;rft.au=Rijke%2C+Maarten+D&amp;rft_id=http%3A%2F%2Fwww.jmlr.org%2Fproceedings%2Fpapers%2Fv32%2Fzoghi14.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Gajane2015ICML:REX3-53"><span class="mw-cite-backlink"><b><a href="#cite_ref-Gajane2015ICML:REX3_53-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFGajaneUrvoyClérot2015">Gajane, Pratik; Urvoy, Tanguy; Clérot, Fabrice (2015), <a class="external text" href="http://jmlr.org/proceedings/papers/v37/gajane15.pdf" rel="nofollow">"A Relative Exponential Weighing Algorithm for Adversarial Utility-based Dueling Bandits"</a> <span class="cs1-format">(PDF)</span>, <i>Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</i></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=A+Relative+Exponential+Weighing+Algorithm+for+Adversarial+Utility-based+Dueling+Bandits&amp;rft.btitle=Proceedings+of+the+32nd+International+Conference+on+Machine+Learning+%28ICML-15%29&amp;rft.date=2015&amp;rft.aulast=Gajane&amp;rft.aufirst=Pratik&amp;rft.au=Urvoy%2C+Tanguy&amp;rft.au=Cl%C3%A9rot%2C+Fabrice&amp;rft_id=http%3A%2F%2Fjmlr.org%2Fproceedings%2Fpapers%2Fv37%2Fgajane15.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Zoghi2015NIPS:CDB-54"><span class="mw-cite-backlink"><b><a href="#cite_ref-Zoghi2015NIPS:CDB_54-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFZoghiKarninWhitesonRijke2015">Zoghi, Masrour; Karnin, Zohar S; Whiteson, Shimon; Rijke, Maarten D (2015), "Copeland Dueling Bandits", <i>Advances in Neural Information Processing Systems, NIPS'15</i>, <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1506.00312" rel="nofollow">1506.00312</a></span>, <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="http://adsabs.harvard.edu/abs/2015arXiv150600312Z" rel="nofollow">2015arXiv150600312Z</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Copeland+Dueling+Bandits&amp;rft.btitle=Advances+in+Neural+Information+Processing+Systems%2C+NIPS%2715&amp;rft.date=2015&amp;rft_id=info%3Aarxiv%2F1506.00312&amp;rft_id=info%3Abibcode%2F2015arXiv150600312Z&amp;rft.aulast=Zoghi&amp;rft.aufirst=Masrour&amp;rft.au=Karnin%2C+Zohar+S&amp;rft.au=Whiteson%2C+Shimon&amp;rft.au=Rijke%2C+Maarten+D&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Komiyama2015COLT:DB-55"><span class="mw-cite-backlink"><b><a href="#cite_ref-Komiyama2015COLT:DB_55-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFKomiyamaHondaKashimaNakagawa2015">Komiyama, Junpei; Honda, Junya; Kashima, Hisashi; Nakagawa, Hiroshi (2015), <a class="external text" href="http://jmlr.org/proceedings/papers/v40/Komiyama15.pdf" rel="nofollow">"Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem"</a> <span class="cs1-format">(PDF)</span>, <i>Proceedings of the 28th Conference on Learning Theory</i></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Regret+Lower+Bound+and+Optimal+Algorithm+in+Dueling+Bandit+Problem&amp;rft.btitle=Proceedings+of+the+28th+Conference+on+Learning+Theory&amp;rft.date=2015&amp;rft.aulast=Komiyama&amp;rft.aufirst=Junpei&amp;rft.au=Honda%2C+Junya&amp;rft.au=Kashima%2C+Hisashi&amp;rft.au=Nakagawa%2C+Hiroshi&amp;rft_id=http%3A%2F%2Fjmlr.org%2Fproceedings%2Fpapers%2Fv40%2FKomiyama15.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-Wu2016DTS-56"><span class="mw-cite-backlink"><b><a href="#cite_ref-Wu2016DTS_56-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFWuLiu2016">Wu, Huasen; Liu, Xin (2016), "Double Thompson Sampling for Dueling Bandits", <i>The 30th Annual Conference on Neural Information Processing Systems (NIPS)</i>, <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1604.07101" rel="nofollow">1604.07101</a></span>, <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="http://adsabs.harvard.edu/abs/2016arXiv160407101W" rel="nofollow">2016arXiv160407101W</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+30th+Annual+Conference+on+Neural+Information+Processing+Systems+%28NIPS%29&amp;rft.atitle=Double+Thompson+Sampling+for+Dueling+Bandits&amp;rft.date=2016&amp;rft_id=info%3Aarxiv%2F1604.07101&amp;rft_id=info%3Abibcode%2F2016arXiv160407101W&amp;rft.aulast=Wu&amp;rft.aufirst=Huasen&amp;rft.au=Liu%2C+Xin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-LKG2016COFIBA-57"><span class="mw-cite-backlink">^ <a href="#cite_ref-LKG2016COFIBA_57-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-LKG2016COFIBA_57-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">
<cite class="citation" id="CITEREFLiAlexandrosGentile2016">Li, Shuai; Alexandros, Karatzoglou; Gentile, Claudio (2016), "Collaborative Filtering Bandits", <i>The 39th International ACM SIGIR Conference on Information Retrieval (SIGIR 2016)</i>, <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1502.03473" rel="nofollow">1502.03473</a></span>, <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="http://adsabs.harvard.edu/abs/2015arXiv150203473L" rel="nofollow">2015arXiv150203473L</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Collaborative+Filtering+Bandits&amp;rft.btitle=The+39th+International+ACM+SIGIR+Conference+on+Information+Retrieval+%28SIGIR+2016%29&amp;rft.date=2016&amp;rft_id=info%3Aarxiv%2F1502.03473&amp;rft_id=info%3Abibcode%2F2015arXiv150203473L&amp;rft.aulast=Li&amp;rft.aufirst=Shuai&amp;rft.au=Alexandros%2C+Karatzoglou&amp;rft.au=Gentile%2C+Claudio&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-GLZ2014CLUB-58"><span class="mw-cite-backlink"><b><a href="#cite_ref-GLZ2014CLUB_58-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFGentileLiZappella2014">Gentile, Claudio; Li, Shuai; Zappella, Giovanni (2014), "Online Clustering of Bandits", <i>The 31st International Conference on Machine Learning, Journal of Machine Learning Research (ICML 2014)</i>, <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1401.8257" rel="nofollow">1401.8257</a></span>, <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a class="external text" href="http://adsabs.harvard.edu/abs/2014arXiv1401.8257G" rel="nofollow">2014arXiv1401.8257G</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Online+Clustering+of+Bandits&amp;rft.btitle=The+31st+International+Conference+on+Machine+Learning%2C+Journal+of+Machine+Learning+Research+%28ICML+2014%29&amp;rft.date=2014&amp;rft_id=info%3Aarxiv%2F1401.8257&amp;rft_id=info%3Abibcode%2F2014arXiv1401.8257G&amp;rft.aulast=Gentile&amp;rft.aufirst=Claudio&amp;rft.au=Li%2C+Shuai&amp;rft.au=Zappella%2C+Giovanni&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-gai2010learning-59"><span class="mw-cite-backlink"><b><a href="#cite_ref-gai2010learning_59-0">^</a></b></span> <span class="reference-text">
<cite class="citation" id="CITEREFGai,_Y._and_Krishnamachari,_B._and_Jain,_R.2010">Gai, Y. and Krishnamachari, B. and Jain, R. (2010), <i>Learning multiuser channel allocations in cognitive radio networks: A combinatorial multi-armed bandit formulation</i>, pp. 1–9</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Learning+multiuser+channel+allocations+in+cognitive+radio+networks%3A+A+combinatorial+multi-armed+bandit+formulation&amp;rft.pages=1-9&amp;rft.date=2010&amp;rft.au=Gai%2C+Y.+and+Krishnamachari%2C+B.+and+Jain%2C+R.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><span class="cs1-maint citation-comment">CS1 maint: Multiple names: authors list (<a href="/wiki/Category:CS1_maint:_Multiple_names:_authors_list" title="Category:CS1 maint: Multiple names: authors list">link</a>)</span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-chen2013combinatorial-60"><span class="mw-cite-backlink">^ <a href="#cite_ref-chen2013combinatorial_60-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-chen2013combinatorial_60-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">
<cite class="citation" id="CITEREFChen,_Wei_and_Wang,_Yajun_and_Yuan,_Yang2013">Chen, Wei and Wang, Yajun and Yuan, Yang (2013), <i>Combinatorial multi-armed bandit: General framework and applications</i>, pp. 151–159</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Combinatorial+multi-armed+bandit%3A+General+framework+and+applications&amp;rft.pages=151-159&amp;rft.date=2013&amp;rft.au=Chen%2C+Wei+and+Wang%2C+Yajun+and+Yuan%2C+Yang&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><span class="cs1-maint citation-comment">CS1 maint: Multiple names: authors list (<a href="/wiki/Category:CS1_maint:_Multiple_names:_authors_list" title="Category:CS1 maint: Multiple names: authors list">link</a>)</span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
<li id="cite_note-ontanon2017combinatorial-61"><span class="mw-cite-backlink">^ <a href="#cite_ref-ontanon2017combinatorial_61-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ontanon2017combinatorial_61-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">
<cite class="citation" id="CITEREFSantiago_Ontañón2017">Santiago Ontañón (2017), "Combinatorial Multi-armed Bandits for Real-Time Strategy Games", <i>Journal of Artificial Intelligence Research</i>, <b>58</b>: 665–702</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Artificial+Intelligence+Research&amp;rft.atitle=Combinatorial+Multi-armed+Bandits+for+Real-Time+Strategy+Games&amp;rft.volume=58&amp;rft.pages=665-702&amp;rft.date=2017&amp;rft.au=Santiago+Onta%C3%B1%C3%B3n&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></span>
</li>
</ol></div>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=35" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><cite class="citation journal">Guha, S.; Munagala, K.; Shi, P. (2010). "Approximation algorithms for restless bandit problems". <i>Journal of the ACM</i>. <b>58</b>: 1–50. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/0711.3861" rel="nofollow">0711.3861</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1145%2F1870103.1870106" rel="nofollow">10.1145/1870103.1870106</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+ACM&amp;rft.atitle=Approximation+algorithms+for+restless+bandit+problems&amp;rft.volume=58&amp;rft.pages=1-50&amp;rft.date=2010&amp;rft_id=info%3Aarxiv%2F0711.3861&amp;rft_id=info%3Adoi%2F10.1145%2F1870103.1870106&amp;rft.aulast=Guha&amp;rft.aufirst=S.&amp;rft.au=Munagala%2C+K.&amp;rft.au=Shi%2C+P.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></li>
<li><cite class="citation" id="CITEREFDayanikPowellYamazaki2008">Dayanik, S.; Powell, W.; Yamazaki, K. (2008), "Index policies for discounted bandit problems with availability constraints", <i>Advances in Applied Probability</i>, <b>40</b> (2): 377–400, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1239%2Faap%2F1214950209" rel="nofollow">10.1239/aap/1214950209</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Applied+Probability&amp;rft.atitle=Index+policies+for+discounted+bandit+problems+with+availability+constraints&amp;rft.volume=40&amp;rft.issue=2&amp;rft.pages=377-400&amp;rft.date=2008&amp;rft_id=info%3Adoi%2F10.1239%2Faap%2F1214950209&amp;rft.aulast=Dayanik&amp;rft.aufirst=S.&amp;rft.au=Powell%2C+W.&amp;rft.au=Yamazaki%2C+K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/>.</li>
<li><cite class="citation" id="CITEREFPowell2007">Powell, Warren B. (2007), "Chapter 10", <i>Approximate Dynamic Programming: Solving the Curses of Dimensionality</i>, New York: John Wiley and Sons, <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0-470-17155-4" title="Special:BookSources/978-0-470-17155-4"><bdi>978-0-470-17155-4</bdi></a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Chapter+10&amp;rft.btitle=Approximate+Dynamic+Programming%3A+Solving+the+Curses+of+Dimensionality&amp;rft.place=New+York&amp;rft.pub=John+Wiley+and+Sons&amp;rft.date=2007&amp;rft.isbn=978-0-470-17155-4&amp;rft.aulast=Powell&amp;rft.aufirst=Warren+B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/>.</li>
<li><cite class="citation" id="CITEREFRobbins1952"><a href="/wiki/Herbert_Robbins" title="Herbert Robbins">Robbins, H.</a> (1952), "Some aspects of the sequential design of experiments", <i><a href="/wiki/Bulletin_of_the_American_Mathematical_Society" title="Bulletin of the American Mathematical Society">Bulletin of the American Mathematical Society</a></i>, <b>58</b> (5): 527–535, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1090%2FS0002-9904-1952-09620-8" rel="nofollow">10.1090/S0002-9904-1952-09620-8</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bulletin+of+the+American+Mathematical+Society&amp;rft.atitle=Some+aspects+of+the+sequential+design+of+experiments&amp;rft.volume=58&amp;rft.issue=5&amp;rft.pages=527-535&amp;rft.date=1952&amp;rft_id=info%3Adoi%2F10.1090%2FS0002-9904-1952-09620-8&amp;rft.aulast=Robbins&amp;rft.aufirst=H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/>.</li>
<li><cite class="citation" id="CITEREFSuttonBarto1998">Sutton, Richard; Barto, Andrew (1998), <a class="external text" href="https://web.archive.org/web/20131211192714/http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" rel="nofollow"><i>Reinforcement Learning</i></a>, MIT Press, <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0-262-19398-6" title="Special:BookSources/978-0-262-19398-6"><bdi>978-0-262-19398-6</bdi></a>, archived from <a class="external text" href="http://webdocs.cs.ualberta.ca/~sutton/book/the-book.html" rel="nofollow">the original</a> on 2013-12-11</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Reinforcement+Learning&amp;rft.pub=MIT+Press&amp;rft.date=1998&amp;rft.isbn=978-0-262-19398-6&amp;rft.aulast=Sutton&amp;rft.aufirst=Richard&amp;rft.au=Barto%2C+Andrew&amp;rft_id=http%3A%2F%2Fwebdocs.cs.ualberta.ca%2F~sutton%2Fbook%2Fthe-book.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/>.</li></ul>
<ul><li><cite class="citation" id="CITEREFAllesiardo2014">Allesiardo, Robin (2014), "A Neural Networks Committee for the Contextual Bandit Problem", <i>Neural Information Processing – 21st International Conference, ICONIP 2014, Malaisia, November 03-06,2014, Proceedings</i>, Lecture Notes in Computer Science, <b>8834</b>, Springer, pp. 374–381, <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a class="external text" href="//arxiv.org/abs/1409.8191" rel="nofollow">1409.8191</a></span>, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1007%2F978-3-319-12637-1_47" rel="nofollow">10.1007/978-3-319-12637-1_47</a>, <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-3-319-12636-4" title="Special:BookSources/978-3-319-12636-4"><bdi>978-3-319-12636-4</bdi></a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=A+Neural+Networks+Committee+for+the+Contextual+Bandit+Problem&amp;rft.btitle=Neural+Information+Processing+%E2%80%93+21st+International+Conference%2C+ICONIP+2014%2C+Malaisia%2C+November+03-06%2C2014%2C+Proceedings&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=374-381&amp;rft.pub=Springer&amp;rft.date=2014&amp;rft_id=info%3Aarxiv%2F1409.8191&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-319-12637-1_47&amp;rft.isbn=978-3-319-12636-4&amp;rft.aulast=Allesiardo&amp;rft.aufirst=Robin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/>.</li></ul>
<ul><li><cite class="citation" id="CITEREFBouneffouf2012">Bouneffouf, Djallel (2012), "A Contextual-Bandit Algorithm for Mobile Context-Aware Recommender System", <i>Neural Information Processing – 19th International Conference, ICONIP 2012, Doha, Qatar, November 12–15,2012, Proceedings, Part III</i>, Lecture Notes in Computer Science, <b>7665</b>, Springer, pp. 324–331, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1007%2F978-3-642-34487-9_40" rel="nofollow">10.1007/978-3-642-34487-9_40</a>, <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-3-642-34486-2" title="Special:BookSources/978-3-642-34486-2"><bdi>978-3-642-34486-2</bdi></a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=A+Contextual-Bandit+Algorithm+for+Mobile+Context-Aware+Recommender+System&amp;rft.btitle=Neural+Information+Processing+%E2%80%93+19th+International+Conference%2C+ICONIP+2012%2C+Doha%2C+Qatar%2C+November+12%E2%80%9315%2C2012%2C+Proceedings%2C+Part+III&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=324-331&amp;rft.pub=Springer&amp;rft.date=2012&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-34487-9_40&amp;rft.isbn=978-3-642-34486-2&amp;rft.aulast=Bouneffouf&amp;rft.aufirst=Djallel&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/>.</li>
<li><cite class="citation" id="CITEREFWeber1992">Weber, Richard (1992), "On the Gittins index for multiarmed bandits", <i><a href="/wiki/Annals_of_Applied_Probability" title="Annals of Applied Probability">Annals of Applied Probability</a></i>, <b>2</b> (4): 1024–1033, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1214%2Faoap%2F1177005588" rel="nofollow">10.1214/aoap/1177005588</a>, <a href="/wiki/JSTOR" title="JSTOR">JSTOR</a> <a class="external text" href="//www.jstor.org/stable/2959678" rel="nofollow">2959678</a></cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annals+of+Applied+Probability&amp;rft.atitle=On+the+Gittins+index+for+multiarmed+bandits&amp;rft.volume=2&amp;rft.issue=4&amp;rft.pages=1024-1033&amp;rft.date=1992&amp;rft_id=info%3Adoi%2F10.1214%2Faoap%2F1177005588&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F2959678&amp;rft.aulast=Weber&amp;rft.aufirst=Richard&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/>.</li>
<li><cite class="citation" id="CITEREFKatehakis,_M._and_C._Derman1986"><a class="mw-redirect" href="/wiki/Michael_N._Katehakis" title="Michael N. Katehakis">Katehakis, M.</a> and C. Derman (1986), "Computing Optimal Sequential Allocation Rules in Clinical Trials", <i>IMS Lecture Notes-Monograph Series</i>, Institute of Mathematical Statistics Lecture Notes - Monograph Series, <b>8</b>: 29–39, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1214%2Flnms%2F1215540286" rel="nofollow">10.1214/lnms/1215540286</a>, <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a> <a href="/wiki/Special:BookSources/978-0-940600-09-6" title="Special:BookSources/978-0-940600-09-6"><bdi>978-0-940600-09-6</bdi></a>, <a href="/wiki/JSTOR" title="JSTOR">JSTOR</a> <a class="external text" href="//www.jstor.org/stable/4355518" rel="nofollow">4355518</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IMS+Lecture+Notes-Monograph+Series&amp;rft.atitle=Computing+Optimal+Sequential+Allocation+Rules+in+Clinical+Trials&amp;rft.volume=8&amp;rft.pages=29-39&amp;rft.date=1986&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F4355518&amp;rft_id=info%3Adoi%2F10.1214%2Flnms%2F1215540286&amp;rft.isbn=978-0-940600-09-6&amp;rft.au=Katehakis%2C+M.+and+C.+Derman&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></li>
<li><cite class="citation" id="CITEREFKatehakis,_M._and_A._F._Veinott,_Jr.1987"><a class="mw-redirect" href="/wiki/Michael_N._Katehakis" title="Michael N. Katehakis">Katehakis, M.</a> and  A. F. Veinott, Jr. (1987), "The multi-armed bandit problem: decomposition and computation", <i>Mathematics of Operations Research</i>, <b>12</b> (2): 262–268, <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a class="external text" href="//doi.org/10.1287%2Fmoor.12.2.262" rel="nofollow">10.1287/moor.12.2.262</a>, <a href="/wiki/JSTOR" title="JSTOR">JSTOR</a> <a class="external text" href="//www.jstor.org/stable/3689689" rel="nofollow">3689689</a>.</cite><span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Mathematics+of+Operations+Research&amp;rft.atitle=The+multi-armed+bandit+problem%3A+decomposition+and+computation&amp;rft.volume=12&amp;rft.issue=2&amp;rft.pages=262-268&amp;rft.date=1987&amp;rft_id=info%3Adoi%2F10.1287%2Fmoor.12.2.262&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F3689689&amp;rft.au=Katehakis%2C+M.+and++A.+F.+Veinott%2C+Jr.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AMulti-armed+bandit"></span><span class="cs1-maint citation-comment">CS1 maint: Multiple names: authors list (<a href="/wiki/Category:CS1_maint:_Multiple_names:_authors_list" title="Category:CS1 maint: Multiple names: authors list">link</a>)</span><link href="mw-data:TemplateStyles:r886058088" rel="mw-deduplicated-inline-style"/></li></ul>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Multi-armed_bandit&amp;action=edit&amp;section=36" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a class="external text" href="http://mloss.org/software/view/415/" rel="nofollow">PyMaBandits</a>, <a class="mw-redirect" href="/wiki/Open-Source" title="Open-Source">open source</a> implementation of bandit strategies in Python and Matlab.</li>
<li><a class="external text" href="https://github.com/Nth-iteration-labs/contextual" rel="nofollow">Contextual</a>, <a href="/wiki/Open_source" title="Open source">open source</a> <a href="/wiki/R_(programming_language)" title="R (programming language)">R</a> package facilitating the simulation and evaluation of both context-free and contextual Multi-Armed Bandit policies.</li>
<li><a class="external text" href="http://bandit.sourceforge.net" rel="nofollow">bandit.sourceforge.net Bandit project</a>, open source implementation of bandit strategies.</li>
<li><a class="external text" href="https://github.com/jkomiyama/banditlib" rel="nofollow">Banditlib</a>, <a class="mw-redirect" href="/wiki/Open-Source" title="Open-Source">Open-Source</a> implementation of bandit strategies in C++.</li>
<li><a class="external text" href="https://archive.is/20121212095047/http://www.cs.washington.edu/research/jair/volume4/kaelbling96a-html/node6.html" rel="nofollow">Leslie Pack Kaelbling and Michael L. Littman (1996). Exploitation versus Exploration: The Single-State Case</a>.</li>
<li>Tutorial: Introduction to Bandits: Algorithms and Theory. <a class="external text" href="http://techtalks.tv/talks/54451/" rel="nofollow">Part1</a>. <a class="external text" href="http://techtalks.tv/talks/54455/" rel="nofollow">Part2</a>.</li>
<li><a class="external text" href="http://www.feynmanlectures.info/exercises/Feynmans_restaurant_problem.html" rel="nofollow">Feynman's restaurant problem</a>, a classic example (with known answer) of the exploitation vs. exploration tradeoff.</li>
<li><a class="external text" href="http://www.chrisstucchio.com/blog/2012/bandit_algorithms_vs_ab.html" rel="nofollow">Bandit algorithms vs. A-B testing</a>.</li>
<li><a class="external text" href="http://homes.di.unimi.it/~cesabian/Pubblicazioni/banditSurvey.pdf" rel="nofollow">S. Bubeck and N. Cesa-Bianchi A Survey on Bandits</a>.</li>
<li><a class="external text" href="https://arxiv.org/abs/1508.03326" rel="nofollow">A Survey on Contextual Multi-armed Bandits</a>, a survey/tutorial for Contextual Bandits.</li>
<li><a class="external text" href="https://mpatacchiola.github.io/blog/2017/08/14/dissecting-reinforcement-learning-6.html" rel="nofollow">Blog post on multi-armed bandit strategies, with Python code</a>.</li>
<li><a class="external text" href="https://pavlov.tech/2019/03/02/animated-multi-armed-bandit-policies/" rel="nofollow">Animated, interactive plots</a> illustrating Epsilon-greedy, <a href="/wiki/Thompson_sampling" title="Thompson sampling">Thompson sampling</a>, and Upper Confidence Bound exploration/exploitation balancing strategies.</li></ul>
<!-- 
NewPP limit report
Parsed by mw1254
Cached time: 20190606104916
Cache expiry: 2592000
Dynamic content: false
CPU time usage: 0.888 seconds
Real time usage: 1.139 seconds
Preprocessor visited node count: 3502/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 121610/2097152 bytes
Template argument size: 940/2097152 bytes
Highest expansion depth: 11/40
Expensive parser function count: 8/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 191932/5000000 bytes
Number of Wikibase entities loaded: 7/400
Lua time usage: 0.547/10.000 seconds
Lua memory usage: 5.46 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  817.844      1 -total
 51.55%  421.626     45 Template:Citation
 23.77%  194.425     14 Template:Cite_journal
  9.76%   79.794      2 Template:Citation_needed
  6.99%   57.174      2 Template:Fix
  4.39%   35.897      4 Template:Category_handler
  2.85%   23.319      3 Template:Cite_book
  2.03%   16.573      2 Template:Delink
  0.52%    4.280      2 Template:Fix/category
-->
<!-- Saved in parser cache with key enwiki:pcache:idhash:2854828-0!canonical!math=5 and timestamp 20190606104915 and revision id 891844157
 -->
</div><noscript><img alt="" height="1" src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" title="" width="1"/></noscript></div>
<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Multi-armed_bandit&amp;oldid=891844157">https://en.wikipedia.org/w/index.php?title=Multi-armed_bandit&amp;oldid=891844157</a>"</div>
<div class="catlinks" data-mw="interface" id="catlinks"><div class="mw-normal-catlinks" id="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Sequential_methods" title="Category:Sequential methods">Sequential methods</a></li><li><a href="/wiki/Category:Sequential_experiments" title="Category:Sequential experiments">Sequential experiments</a></li><li><a href="/wiki/Category:Stochastic_optimization" title="Category:Stochastic optimization">Stochastic optimization</a></li><li><a href="/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></div><div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">Hidden categories: <ul><li><a href="/wiki/Category:CS1_errors:_chapter_ignored" title="Category:CS1 errors: chapter ignored">CS1 errors: chapter ignored</a></li><li><a href="/wiki/Category:CS1_maint:_Multiple_names:_authors_list" title="Category:CS1 maint: Multiple names: authors list">CS1 maint: Multiple names: authors list</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_March_2015" title="Category:Articles with unsourced statements from March 2015">Articles with unsourced statements from March 2015</a></li></ul></div></div>
<div class="visualClear"></div>
</div>
</div>
<div id="mw-navigation">
<h2>Navigation menu</h2>
<div id="mw-head">
<div aria-labelledby="p-personal-label" id="p-personal" role="navigation">
<h3 id="p-personal-label">Personal tools</h3>
<ul>
<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a accesskey="n" href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]">Talk</a></li><li id="pt-anoncontribs"><a accesskey="y" href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Multi-armed+bandit" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a accesskey="o" href="/w/index.php?title=Special:UserLogin&amp;returnto=Multi-armed+bandit" title="You're encouraged to log in; however, it's not mandatory. [o]">Log in</a></li> </ul>
</div>
<div id="left-navigation">
<div aria-labelledby="p-namespaces-label" class="vectorTabs" id="p-namespaces" role="navigation">
<h3 id="p-namespaces-label">Namespaces</h3>
<ul>
<li class="selected" id="ca-nstab-main"><span><a accesskey="c" href="/wiki/Multi-armed_bandit" title="View the content page [c]">Article</a></span></li><li id="ca-talk"><span><a accesskey="t" href="/wiki/Talk:Multi-armed_bandit" rel="discussion" title="Discussion about the content page [t]">Talk</a></span></li> </ul>
</div>
<div aria-labelledby="p-variants-label" class="vectorMenu emptyPortlet" id="p-variants" role="navigation">
<input aria-labelledby="p-variants-label" class="vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-variants-label">
<span>Variants</span>
</h3>
<ul class="menu">
</ul>
</div>
</div>
<div id="right-navigation">
<div aria-labelledby="p-views-label" class="vectorTabs" id="p-views" role="navigation">
<h3 id="p-views-label">Views</h3>
<ul>
<li class="collapsible selected" id="ca-view"><span><a href="/wiki/Multi-armed_bandit">Read</a></span></li><li class="collapsible" id="ca-edit"><span><a accesskey="e" href="/w/index.php?title=Multi-armed_bandit&amp;action=edit" title="Edit this page [e]">Edit</a></span></li><li class="collapsible" id="ca-history"><span><a accesskey="h" href="/w/index.php?title=Multi-armed_bandit&amp;action=history" title="Past revisions of this page [h]">View history</a></span></li> </ul>
</div>
<div aria-labelledby="p-cactions-label" class="vectorMenu emptyPortlet" id="p-cactions" role="navigation">
<input aria-labelledby="p-cactions-label" class="vectorMenuCheckbox" type="checkbox"/>
<h3 id="p-cactions-label"><span>More</span></h3>
<ul class="menu">
</ul>
</div>
<div id="p-search" role="search">
<h3>
<label for="searchInput">Search</label>
</h3>
<form action="/w/index.php" id="searchform">
<div id="simpleSearch">
<input accesskey="f" id="searchInput" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" type="search"/><input name="title" type="hidden" value="Special:Search"/><input class="searchButton mw-fallbackSearchButton" id="mw-searchButton" name="fulltext" title="Search Wikipedia for this text" type="submit" value="Search"/><input class="searchButton" id="searchButton" name="go" title="Go to a page with this exact name if it exists" type="submit" value="Go"/> </div>
</form>
</div>
</div>
</div>
<div id="mw-panel">
<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a></div>
<div aria-labelledby="p-navigation-label" class="portal" id="p-navigation" role="navigation">
<h3 id="p-navigation-label">Navigation</h3>
<div class="body">
<ul>
<li id="n-mainpage-description"><a accesskey="z" href="/wiki/Main_Page" title="Visit the main page [z]">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a accesskey="x" href="/wiki/Special:Random" title="Load a random article [x]">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-interaction-label" class="portal" id="p-interaction" role="navigation">
<h3 id="p-interaction-label">Interaction</h3>
<div class="body">
<ul>
<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a accesskey="r" href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-tb-label" class="portal" id="p-tb" role="navigation">
<h3 id="p-tb-label">Tools</h3>
<div class="body">
<ul>
<li id="t-whatlinkshere"><a accesskey="j" href="/wiki/Special:WhatLinksHere/Multi-armed_bandit" title="List of all English Wikipedia pages containing links to this page [j]">What links here</a></li><li id="t-recentchangeslinked"><a accesskey="k" href="/wiki/Special:RecentChangesLinked/Multi-armed_bandit" rel="nofollow" title="Recent changes in pages linked from this page [k]">Related changes</a></li><li id="t-upload"><a accesskey="u" href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]">Upload file</a></li><li id="t-specialpages"><a accesskey="q" href="/wiki/Special:SpecialPages" title="A list of all special pages [q]">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Multi-armed_bandit&amp;oldid=891844157" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Multi-armed_bandit&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a accesskey="g" href="https://www.wikidata.org/wiki/Special:EntityPage/Q2882343" title="Link to connected data repository item [g]">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Multi-armed_bandit&amp;id=891844157" title="Information on how to cite this page">Cite this page</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-coll-print_export-label" class="portal" id="p-coll-print_export" role="navigation">
<h3 id="p-coll-print_export-label">Print/export</h3>
<div class="body">
<ul>
<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Multi-armed+bandit">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Multi-armed+bandit&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a accesskey="p" href="/w/index.php?title=Multi-armed_bandit&amp;printable=yes" title="Printable version of this page [p]">Printable version</a></li> </ul>
</div>
</div>
<div aria-labelledby="p-lang-label" class="portal" id="p-lang" role="navigation">
<h3 id="p-lang-label">Languages</h3>
<div class="body">
<ul>
<li class="interlanguage-link interwiki-ca"><a class="interlanguage-link-target" href="https://ca.wikipedia.org/wiki/El_problema_de_la_m%C3%A0quina_escurabutxaques" hreflang="ca" lang="ca" title="El problema de la màquina escurabutxaques – Catalan">Català</a></li><li class="interlanguage-link interwiki-es"><a class="interlanguage-link-target" href="https://es.wikipedia.org/wiki/Bandido_multibrazo" hreflang="es" lang="es" title="Bandido multibrazo – Spanish">Español</a></li><li class="interlanguage-link interwiki-fr"><a class="interlanguage-link-target" href="https://fr.wikipedia.org/wiki/Bandit_manchot_(math%C3%A9matiques)" hreflang="fr" lang="fr" title="Bandit manchot (mathématiques) – French">Français</a></li> </ul>
<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a class="wbc-editpage" href="https://www.wikidata.org/wiki/Special:EntityPage/Q2882343#sitelinks-wikipedia" title="Edit interlanguage links">Edit links</a></span></div> </div>
</div>
</div>
</div>
<div id="footer" role="contentinfo">
<ul id="footer-info">
<li id="footer-info-lastmod"> This page was last edited on 10 April 2019, at 14:16<span class="anonymous-show"> (UTC)</span>.</li>
<li id="footer-info-copyright">Text is available under the <a href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" rel="license">Creative Commons Attribution-ShareAlike License</a><a href="//creativecommons.org/licenses/by-sa/3.0/" rel="license" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
</ul>
<ul id="footer-places">
<li id="footer-places-privacy"><a class="extiw" href="https://foundation.wikimedia.org/wiki/Privacy_policy" title="wmf:Privacy policy">Privacy policy</a></li>
<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
<li id="footer-places-mobileview"><a class="noprint stopMobileRedirectToggle" href="//en.m.wikipedia.org/w/index.php?title=Multi-armed_bandit&amp;mobileaction=toggle_view_mobile">Mobile view</a></li>
</ul>
<ul class="noprint" id="footer-icons">
<li id="footer-copyrightico">
<a href="https://wikimediafoundation.org/"><img alt="Wikimedia Foundation" height="31" src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88"/></a> </li>
<li id="footer-poweredbyico">
<a href="https://www.mediawiki.org/"><img alt="Powered by MediaWiki" height="31" src="/static/images/poweredby_mediawiki_88x31.png" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88"/></a> </li>
</ul>
<div style="clear: both;"></div>
</div>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.888","walltime":"1.139","ppvisitednodes":{"value":3502,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":121610,"limit":2097152},"templateargumentsize":{"value":940,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":8,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":191932,"limit":5000000},"entityaccesscount":{"value":7,"limit":400},"timingprofile":["100.00%  817.844      1 -total"," 51.55%  421.626     45 Template:Citation"," 23.77%  194.425     14 Template:Cite_journal","  9.76%   79.794      2 Template:Citation_needed","  6.99%   57.174      2 Template:Fix","  4.39%   35.897      4 Template:Category_handler","  2.85%   23.319      3 Template:Cite_book","  2.03%   16.573      2 Template:Delink","  0.52%    4.280      2 Template:Fix/category"]},"scribunto":{"limitreport-timeusage":{"value":"0.547","limit":"10.000"},"limitreport-memusage":{"value":5720174,"limit":52428800}},"cachereport":{"origin":"mw1254","timestamp":"20190606104916","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Multi-armed bandit","url":"https:\/\/en.wikipedia.org\/wiki\/Multi-armed_bandit","sameAs":"http:\/\/www.wikidata.org\/entity\/Q2882343","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q2882343","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2005-10-07T14:38:28Z","dateModified":"2019-04-10T14:16:41Z","image":"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/8\/82\/Las_Vegas_slot_machines.jpg"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":1287,"wgHostname":"mw1254"});});</script>
</body>
</html>
